{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем <B>\"Facebook SDK\"</B> для более удобного общения с <i>fecabook.com</i>, а так же модуль <b>\"requests\"</b> для того, что бы работать с <i>HTTP</i> запросами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем классы утилит, в которых скрыта работа с <b>БД \"Elasticsearch\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же для измирения времени работы некоторых участоков подключим модуль <b>\"time\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time():\n",
    "    return int(round(t.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемые константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FACEBOOK_TOKEN = \"1769775703259571|736fc7f9c5dc31707d40709a1d37813b\"\n",
    "FACEBOOK_POSTS_COUNT = 3000\n",
    "\n",
    "ES_INDEX = \"fb_group_posts\"\n",
    "ES_POSTS_DOC_TYPE = \"post\"\n",
    "ES_NAMES_RELATIONS_DOC_TYPE = \"name_relations\"\n",
    "ES_BULK_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сущьность описывающая заведомо известные данные о группе в <b>Facebook</b>:\n",
    "* Имя\n",
    "* Id на facebook.com\n",
    "* Домен (класс) тематики группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    def __init__(self, name, id, domain):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.domain = domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список групп, которые будут использованы для работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "#     Group(name=\"CNN Politics\", id=\"219367258105115\", domain=\"politics\"),\n",
    "#     Group(name=\"SinoRuss\", id=\"1565161760380398\", domain=\"politics\"),\n",
    "#     Group(name=\"Politics & Sociology\", id=\"1616754815303974\", domain=\"politics\"),\n",
    "#     Group(name=\"CNN Money\", id=\"6651543066\", domain=\"finances\"),\n",
    "#     Group(name=\"MTV\", id=\"7245371700\", domain=\"music\"),\n",
    "#     Group(name=\"CNET\", id=\"7155422274\", domain=\"tech\"),\n",
    "#     Group(name=\"TechCrunch\", id=\"8062627951\", domain=\"tech\"),\n",
    "#     Group(name=\"Sport Addicts\", id=\"817513368382866\", domain=\"sport\"),\n",
    "#     Group(name=\"Pokemon GO\", id=\"1745029562403910\", domain=\"pokemon go\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с Facebook будем использовать <b>Facebook Graph API</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from facebook import GraphAPI\n",
    "\n",
    "graph = GraphAPI(access_token=FACEBOOK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же нам понадобится модуль для совершения <b>HTTP</b> запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За хранилище данных возьмем <b>Elasticsearch</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "database = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся работа с Elasticsearch скрыта в модуле <b>facebook_database_helper</b>, и управляется через главный класс <b>SimpleFacebookDBHelper</b>, который кеширует данные необходимые для работы с бд, предоставляя удобный интерфейс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from facebook_database_helper import SimpleFacebookDBHelper\n",
    "\n",
    "fdb = SimpleFacebookDBHelper(\n",
    "    es=database,                                        # Cам Elasticsearch\n",
    "    index=ES_INDEX,                                     # Индекс под которых будут хранится все данные\n",
    "    post_doc_type=ES_POSTS_DOC_TYPE,                    # _type для постов\n",
    "    name_relation_doc_type=ES_NAMES_RELATIONS_DOC_TYPE) # _type для отношений имя <=> пост"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загружающая последовательно посты пока не достигнет предела группы или ограничения по кол-ву."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_next_posts(posts, max_count):\n",
    "    result = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        if count > max_count:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            for post_data in posts['data']:\n",
    "                keys = post_data.keys()  # Так как мы нам нужны сами сообщения\n",
    "                                         # мы не включаем в результат посты без текстовых сообщений\n",
    "                if 'message' in keys:\n",
    "                    result.append(post_data)\n",
    "\n",
    "            # Меняем размер запрашиваемого пака (страницы) с постами, в уже сформированном запросе \n",
    "            # от Facebook Graph API\n",
    "            request = posts['paging']['next'].replace(\"limit=25\", \"limit=100\")\n",
    "\n",
    "            s_time = time()\n",
    "            posts = requests.get(request).json()\n",
    "            f_time = time()\n",
    "            \n",
    "            posts_count = len(posts['data'])\n",
    "            count += posts_count\n",
    "            \n",
    "            print \"Время загрузки пака постов ->\", (f_time - s_time), \"|\", \"кол-во:\", posts_count, \"|\", \"всего:\", count\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    print \"Общее кол-во загруженных постов группы ->\", count\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя ранее описанный список постов заполняем нашу базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    # Получаем данные о группе по ее id на facebook.com\n",
    "    s_time = time()\n",
    "    group_json = graph.get_object(group.id)\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки данных [', group_json['name'], '] группы ->', (f_time - s_time)\n",
    "    \n",
    "    # Получаем первый пак постов, используем уже для этого метод \"get_connections\"\n",
    "    # который, грубо говоря, дает возможность запрашивать списки\n",
    "    s_time = time()\n",
    "    first_posts_pack = graph.get_connections(group_json['id'], 'feed')\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки первого пака постов ->', (f_time - s_time)\n",
    "    \n",
    "    # Последовательно загружаем все последующие посты\n",
    "    posts = load_next_posts(posts=first_posts_pack, max_count=FACEBOOK_POSTS_COUNT)\n",
    "    \n",
    "    # Сохраняем все в базу данных\n",
    "    s_time = time()\n",
    "    fdb.save_posts(group.name, group.domain, posts)\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время сохранение постов в БД ->\", (f_time - s_time)\n",
    "    print \"-------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как двигатся дальше мы должны вытащить все имена, сформировав отношение \"имя <-> пост\", для последующего использования этих отношений при поиске. Логично, что это следует сделать перед дальнейшей обработкой и работы с постами, так как мы можем утратить часть имен в дальнейшем.\n",
    "\n",
    "Для того, что бы отискать все имена, мы воспользуемся регулярными выражениями, в данном случае, самой простой реализацией, которая даст много мусора, но в данном случае это не страшно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FIRST_LAST_NAME_PATTERN = \"[A-Z]{1}[a-z]+\\s+[A-Z]{1}[a-z]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем пользоваться уже сохраненными данными (тем что сохранили ранее в базу данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NameRelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-215fb79d8ef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mrelation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameRelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mname_relations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NameRelation' is not defined"
     ]
    }
   ],
   "source": [
    "posts = fdb.get_all_posts() # Вытаскиваем все посты\n",
    "name_relations = []\n",
    "\n",
    "s_time = time()\n",
    "\n",
    "for post in posts:\n",
    "    message = post['_source']['message']\n",
    "    names = re.findall(FIRST_LAST_NAME_PATTERN, message)\n",
    "\n",
    "    if names:\n",
    "        id = post['_id']\n",
    "\n",
    "        for name in names:\n",
    "            name = name.replace(\".\", \"\")\n",
    "            relation = NameRelation(fl_name=name, post_id=id)\n",
    "            name_relations.append(relation)\n",
    "\n",
    "f_time = time()\n",
    "\n",
    "print \"Время поиска имен по шаблону ->\", (f_time - s_time)\n",
    "    \n",
    "len(name_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку все отношения мы так же будем хранить в базе данных, то очистим ранее созданные связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_time = time()\n",
    "deleted_posts = fdb.delete_all_name_relations()\n",
    "f_time = time()\n",
    "\n",
    "print \"Время удаления старых отношений 'сообщение <=> имя' ->\", (f_time - s_time)\n",
    "print \"Удалено старых постов:\", deleted_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_time = time()\n",
    "fdb.save_name_relations(name_relations)\n",
    "f_time = time()\n",
    "\n",
    "print \"Время сохранения новых отношений 'сообщение <=> имя' ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает поиск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя [ John Kerry ] найдено соответстивий (сообщений): 19\n",
      "Имя [ Paul Reichler ] найдено соответстивий (сообщений): 9\n"
     ]
    }
   ],
   "source": [
    "searched_names = [\n",
    "    \"John Kerry\",\n",
    "    \"Paul Reichler\"\n",
    "]\n",
    "\n",
    "def find_messages_by_fl_name(fl_name):\n",
    "    finded_relations = fdb.get_name_relations_by_fl(fl_name=fl_name)\n",
    "    messages = []\n",
    "    finded_post_ids = set() # Необходимо, что бы учитывать ранее найденные посты\n",
    "\n",
    "    for relation in finded_relations:\n",
    "        post_id = relation['_source']['post_id']\n",
    "\n",
    "        if not post_id in finded_post_ids:\n",
    "            finded_post_ids.add(post_id)\n",
    "            \n",
    "            message = fdb.get_post_by_id(post_id)['_source']['message']\n",
    "            messages.append(message)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "for name in searched_names:\n",
    "    messages = find_messages_by_fl_name(name)\n",
    "    \n",
    "    print \"Имя [\", name, \"] найдено соответстивий (сообщений):\", len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words_loader import StopWordsLoader\n",
    "\n",
    "stop_words = StopWordsLoader(\"stop_words\").get()\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset(['all', 'show', 'anyway', 'four', 'go', 'mill', 'find', 'seemed', 'whose', 're', 'herself', 'whoever', 'behind', 'should', 'to', 'only', 'under', 'herein', 'do', 'his', 'get', 'very', 'de', 'myself', 'cannot', 'every', 'yourselves', 'him', 'is', 'cry', 'beforehand', 'these', 'sh...ho', 'most', 'eight', 'but', 'nothing', 'why', 'noone', 'sometimes', 'together', 'serious', 'once']),\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    return stem_tokens(tokens, stemmer)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    \n",
    "    return stemmed\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18137"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = fdb.get_all_messages()\n",
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52151"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(messages)\n",
    "\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска 100 популярных токенов -> 125252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'http', 7451),\n",
       " (u'com', 3775),\n",
       " (u'https', 3131),\n",
       " (u'cnnmon', 2567),\n",
       " (u'www', 2235),\n",
       " (u'china', 2103),\n",
       " (u'news', 1893),\n",
       " (u'trump', 1846),\n",
       " (u'new', 1824),\n",
       " (u'donald', 1422),\n",
       " (u'media', 1337),\n",
       " (u'russia', 1302),\n",
       " (u'ukraine', 1295),\n",
       " (u'people', 1288),\n",
       " (u'isis', 1259),\n",
       " (u'like', 1250),\n",
       " (u'america', 1205),\n",
       " (u'2016', 1196),\n",
       " (u'world', 1193),\n",
       " (u'just', 1156),\n",
       " (u'clinton', 1098),\n",
       " (u'says', 1096),\n",
       " (u'jpg', 1074),\n",
       " (u'chinese', 1065),\n",
       " (u'youtube', 1053),\n",
       " (u'watch', 1037),\n",
       " (u'syria', 1018),\n",
       " (u'american', 1008),\n",
       " (u'die', 1001),\n",
       " (u'russian', 987),\n",
       " (u'war', 941),\n",
       " (u'cnn', 912),\n",
       " (u'time', 887),\n",
       " (u'hillary', 880),\n",
       " (u'pbs', 865),\n",
       " (u'twimg', 865),\n",
       " (u'en', 804),\n",
       " (u'president', 780),\n",
       " (u'video', 779),\n",
       " (u'der', 756),\n",
       " (u'military', 731),\n",
       " (u'state', 727),\n",
       " (u'today', 719),\n",
       " (u'don', 715),\n",
       " (u'year', 689),\n",
       " (u'forces', 654),\n",
       " (u'army', 654),\n",
       " (u'html', 643),\n",
       " (u'know', 637),\n",
       " (u'und', 634),\n",
       " (u'obama', 629),\n",
       " (u'syrian', 620),\n",
       " (u'make', 617),\n",
       " (u'years', 608),\n",
       " (u'aleppo', 599),\n",
       " (u'ukrainian', 591),\n",
       " (u'killed', 581),\n",
       " (u'good', 572),\n",
       " (u'think', 568),\n",
       " (u'2015', 561),\n",
       " (u'way', 554),\n",
       " (u'al', 546),\n",
       " (u'donbass', 539),\n",
       " (u'turkey', 538),\n",
       " (u'day', 534),\n",
       " (u'facebook', 516),\n",
       " (u'12', 516),\n",
       " (u'south', 512),\n",
       " (u'white', 510),\n",
       " (u'iraq', 505),\n",
       " (u'man', 503),\n",
       " (u'need', 500),\n",
       " (u'want', 482),\n",
       " (u'ly', 478),\n",
       " (u'said', 476),\n",
       " (u'national', 475),\n",
       " (u'women', 474),\n",
       " (u'city', 473),\n",
       " (u'putin', 471),\n",
       " (u'say', 467),\n",
       " (u'right', 465),\n",
       " (u'10', 459),\n",
       " (u'look', 456),\n",
       " (u'campaign', 454),\n",
       " (u'life', 449),\n",
       " (u'country', 449),\n",
       " (u'big', 448),\n",
       " (u'terrorists', 444),\n",
       " (u'bit', 441),\n",
       " (u'money', 439),\n",
       " (u'best', 438),\n",
       " (u'attack', 437),\n",
       " (u'going', 435),\n",
       " (u'near', 433),\n",
       " (u'youtu', 432),\n",
       " (u'party', 424),\n",
       " (u'07', 417),\n",
       " (u'power', 414),\n",
       " (u'militants', 410),\n",
       " (u'live', 408)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_columns_sum(matrix):\n",
    "#     rows_count = matrix.getnnz()\n",
    "    \n",
    "#     if rows_count == 0:\n",
    "#         return \n",
    "    \n",
    "#     first_row = matrix[0]\n",
    "#     columns_count = first_row.getnnz()\n",
    "    \n",
    "#     result = []\n",
    "    \n",
    "#     for i in range(columns_count):\n",
    "#         s_time = time()\n",
    "#         result.append(sum(matrix[:,i])[0,0])\n",
    "#         f_time = time()\n",
    "        \n",
    "#         print \"time ->\", (f_time - s_time)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# summ = get_columns_sum(matrix)\n",
    "# summ\n",
    "\n",
    "def find_top_tokens(matrix, items, amount):\n",
    "    s_time = time()\n",
    "    counts = [(word, matrix.getcol(col_num).sum()) for word, col_num in items]\n",
    "    tokens = sorted (counts, key = lambda x: -x[1])[:min(amount, len(counts))]\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время поиска [\", amount, \"] популярных токенов ->\", (f_time - s_time)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "top_100_tokens,\n",
    "\n",
    "s_time = time()\n",
    "freqs = [(word, matrix.getcol(col_num).sum()) for word, col_num in vectorizer.vocabulary_.items()]\n",
    "top_100_tokens = sorted (freqs, key = lambda x: -x[1])[:100]\n",
    "f_time = time()\n",
    "\n",
    "print \"Время поиска 100 популярных токенов ->\", (f_time - s_time)\n",
    "\n",
    "top_100_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b97f2683def0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msorted_by_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msorted_by_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "sorted_by_count = sorted(tokens.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_by_count[0:100]\n",
    "\n",
    "\n",
    "    \n",
    "#     print \"{:10}\".format(token), \"->\", get_words_count(token, vectorizer.vocabulary_, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
