{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальное задание по практике\n",
    "--------------------\n",
    "\n",
    "### Цель\n",
    "\n",
    "Ознакомится с базовыми алгоритмами и базовыми инструментами машинного обучения для анализа текста\n",
    "\n",
    "### Задание\n",
    "\n",
    "1) Получить категоризованные данные с <b>Facebook API</b><br>\n",
    "2) Сохранить полученные данные<br>\n",
    "3) Токенизация данных<br>\n",
    "4) Удаление стоп-слов<br>\n",
    "5) Стемминг<br>\n",
    "6) Поиск паттерна имя-фамилия в тексте<br>\n",
    "7) Формирование списка топ <i>100</i> важных токенов по каждой категории<br>\n",
    "8) Категоризация новых текстов по наличию в них этих топ <i>100</i> токенов<br>\n",
    "9) Оценка качества<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодировка для кода на <b>Python</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для измирения времени работы некоторых участоков подключим модуль <b>\"time\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "# вернет: текущее время в миллисекундах\n",
    "def time(): return int(round(t.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранение данных\n",
    "\n",
    "Для хранения данных была выбрана база данных <b>Elasticsearch</b>, которая предоставляет удобный интерфейс взаимодействия, а так же кроссплатформенность при работе с ней, за счет того, что взаимодействие осуществляется по HTTP протоколу.\n",
    "\n",
    "Константы работы с базой данных <b>Elasticsearch</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ES_INDEX = \"fb_group_posts\"                 # Аналог базы данных в сравнении с реляционными БД, под ним будут\n",
    "                                            # хранится все данные, необходимые по заданию\n",
    "ES_POST_DOC_TYPE = \"post\"                   \n",
    "ES_NAME_RELATION_DOC_TYPE = \"name_relation\"\n",
    "\n",
    "ES_BULK_ACTIONS_SIZE = 500                  # Размер пака данных отсылаемых за раз в elasticsearch, \n",
    "                                            # необходимо для оптимизации по скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с базой данных не относится к заданию напрямую, по-этому описанию работе с ней уделено меньше внимания. Вся работа с <b>Elasticsearch</b> скрыта в ниже описанном классе <b>FacebookDBHelper</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import TransportError\n",
    "from elasticsearch import helpers\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Дата-класс, который описывает отношение наличия имени в тексте поста\n",
    "# @param: fl_name - Имя Фамилия\n",
    "# @param: post_id - id поста, в тексте которого содержится это имя и фамилия\n",
    "class FLNameData(object):\n",
    "    def __init__(self, fl_name, post_id):\n",
    "        self.fl_name = fl_name\n",
    "        self.post_id = post_id\n",
    "\n",
    "\n",
    "class FacebookDBHelper(object):\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch()\n",
    "    \n",
    "    def save_posts(self, group_name, group_domain, posts):\n",
    "        actions = []\n",
    "\n",
    "        for post in posts:\n",
    "            if 'message' in post.keys():\n",
    "                action = {\n",
    "                    \"_index\": ES_INDEX,\n",
    "                    \"_type\": ES_POST_DOC_TYPE,\n",
    "                    \"_id\": post['id'],\n",
    "                    \"_source\": {\n",
    "                        \"message\": post['message'],\n",
    "                        \"group_name\": group_name,\n",
    "                        \"group_domain\": group_domain\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "        \n",
    "    def save_name_relations(self, relations):\n",
    "        actions = []\n",
    "\n",
    "        for relation in relations:\n",
    "            action = {\n",
    "                \"_index\": ES_INDEX,\n",
    "                \"_type\": ES_NAME_RELATION_DOC_TYPE,\n",
    "                \"_source\": {\n",
    "                    \"fl_name\": relation.fl_name,\n",
    "                    \"post_id\": relation.post_id\n",
    "                }\n",
    "            }\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "\n",
    "    def __bulk_insert(self, actions):\n",
    "        actions_list = split_list(list(actions), ES_BULK_ACTIONS_SIZE)\n",
    "\n",
    "        for acts in actions_list:\n",
    "            helpers.bulk(self.es, acts)\n",
    "\n",
    "    def get_post_by_id(self, id):\n",
    "        return self.__get(doc_type=ES_POST_DOC_TYPE, id=id)\n",
    "\n",
    "    def get_all_posts(self):\n",
    "        return self.__get_all(doc_type=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def get_all_name_relations(self, doc_type):\n",
    "        relations = self.__get_all(doc_type=doc_type)\n",
    "\n",
    "        # noinspection PyTypeChecker\n",
    "        return [FLNameData(fl_name=r['_source']['fl_name'], post_id=r[\"_id\"]) for r in relations]\n",
    "\n",
    "    def __get(self, doc_type, id):\n",
    "        return self.es.get(index=ES_INDEX, doc_type=doc_type, id=id)\n",
    "\n",
    "    def __get_all(self, doc_type, body=None):\n",
    "        if body is None:\n",
    "            body = {}\n",
    "\n",
    "        result = []\n",
    "\n",
    "        page = self.es.search(\n",
    "            index=ES_INDEX,\n",
    "            doc_type=doc_type,\n",
    "            scroll='2m',\n",
    "            search_type='scan',\n",
    "            size=1000,\n",
    "            body=body)\n",
    "\n",
    "        scroll_id = page['_scroll_id']\n",
    "        scroll_size = page['hits']['total']\n",
    "\n",
    "        while scroll_size > 0:\n",
    "            page = self.es.scroll(scroll_id=scroll_id, scroll='2m')\n",
    "\n",
    "            scroll_id = page['_scroll_id']\n",
    "            scroll_size = len(page['hits']['hits'])\n",
    "\n",
    "            result.extend(page['hits']['hits'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_sources(self, doc_type):\n",
    "        posts = self.get_all_posts()\n",
    "        result = []\n",
    "\n",
    "        for post in posts:\n",
    "            if \"_source\" in post.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(post[\"_source\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_messages(self):\n",
    "        sources = self.get_all_sources(doc_type=ES_POST_DOC_TYPE)\n",
    "        result = []\n",
    "\n",
    "        for source in sources:\n",
    "            if 'message' in source.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(source[\"message\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def delete_all_posts(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def delete_all_name_relations(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_NAME_RELATION_DOC_TYPE)\n",
    "\n",
    "    def get_messages_by_domain(self, domain):\n",
    "        posts = self.__get_all(doc_type=ES_POST_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"group_domain\": {\n",
    "                        \"query\": domain,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return [p['_source']['message'] for p in posts]\n",
    "\n",
    "    def get_name_relations_by_fl(self, fl_name):\n",
    "        return self.__get_all(doc_type=ES_NAME_RELATION_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"fl_name\": {\n",
    "                        \"query\": fl_name,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    def get_all_domains(self):\n",
    "        response = self.es.search(index=ES_INDEX, doc_type=ES_POST_DOC_TYPE, body={\n",
    "            \"size\": 0,\n",
    "            \"aggs\": {\n",
    "                \"langs\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"group_domain\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for bucket in response['aggregations']['langs']['buckets']:\n",
    "            result.append(bucket['key'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def delete_by_doc_type(es, index, type_):\n",
    "    try:\n",
    "        count = es.count(index, type_)['count']\n",
    "        max_count = 5000\n",
    "\n",
    "        if not count:\n",
    "            return 0\n",
    "\n",
    "        tmp_count = count\n",
    "\n",
    "        while tmp_count > 0:\n",
    "            tmp_count -= max_count\n",
    "\n",
    "            response = es.search(\n",
    "                index=index,\n",
    "                filter_path=[\"hits.hits._id\"],\n",
    "                body={\"size\": max_count,\n",
    "                      \"query\": {\n",
    "                          \"filtered\": {\n",
    "                              \"filter\": {\n",
    "                                    \"type\": {\"value\": type_}\n",
    "                              }\n",
    "                          }\n",
    "                      }})\n",
    "\n",
    "            if not response:\n",
    "                return 0\n",
    "\n",
    "            ids = [x[\"_id\"] for x in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "            if not ids:\n",
    "                return 0\n",
    "\n",
    "            bulk_body = [\n",
    "                '{{\"delete\": {{\"_index\": \"{}\", \"_type\": \"{}\", \"_id\": \"{}\"}}}}'.format(index, type_, x)\n",
    "                for x in ids]\n",
    "\n",
    "            es.bulk('\\n'.join(bulk_body))\n",
    "            es.indices.flush_synced([index])\n",
    "\n",
    "        return count\n",
    "    except TransportError as ex:\n",
    "        print(\"Elasticsearch error: \" + ex.error)\n",
    "        raise ex\n",
    "        \n",
    "def split_list(list_, count_):\n",
    "    result = []\n",
    "\n",
    "    if len(list_) == 0:\n",
    "        return result\n",
    "\n",
    "    if len(list_) == count_:\n",
    "        result.append(list_)\n",
    "\n",
    "        return result\n",
    "\n",
    "    steps = len(list_) / count_\n",
    "    tmp_list = deepcopy(list_)\n",
    "\n",
    "    for i in range(0, steps):\n",
    "        result.append(tmp_list[0: count_])\n",
    "        tmp_list = tmp_list[count_: len(tmp_list)]\n",
    "\n",
    "    if len(tmp_list) != 0:\n",
    "        result.append(tmp_list)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экземпляр класса <b>FacebookDBHelper</b>, через который мы будем взаимодействовать с <b>Elasticsearch</b> во всей работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdb = FacebookDBHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка постов с facebook.com используя Gaph API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Константы для работы с <b>Facebook API</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Токен зарегестрированного приложения в консоли разработчика на facebook.com\n",
    "FACEBOOK_TOKEN = \"1769775703259571|736fc7f9c5dc31707d40709a1d37813b\"\n",
    "\n",
    "# Кол-во постов загружаемых с одной группы на facebook.com\n",
    "FACEBOOK_POSTS_COUNT = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сущьность описывающая заведомо известные данные о группе в <b>Facebook</b>:\n",
    "* Имя\n",
    "* Id на facebook.com\n",
    "* Домен (класс) тематики группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    def __init__(self, name, id, domain):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.domain = domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список групп, которые будут использованы для работы (с которых мы будем загружать посты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "#     Group(name=\"CNN Politics\", id=\"219367258105115\", domain=\"politics\"),\n",
    "#     Group(name=\"SinoRuss\", id=\"1565161760380398\", domain=\"politics\"),\n",
    "#     Group(name=\"Politics & Sociology\", id=\"1616754815303974\", domain=\"politics\"),\n",
    "#     Group(name=\"CNN Money\", id=\"6651543066\", domain=\"finances\"),\n",
    "#     Group(name=\"MTV\", id=\"7245371700\", domain=\"music\"),\n",
    "#     Group(name=\"CNET\", id=\"7155422274\", domain=\"tech\"),\n",
    "#     Group(name=\"TechCrunch\", id=\"8062627951\", domain=\"tech\"),\n",
    "#     Group(name=\"Sport Addicts\", id=\"817513368382866\", domain=\"sport\"),\n",
    "#     Group(name=\"Pokemon GO\", id=\"1745029562403910\", domain=\"pokemon_go\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с Facebook будем использовать <b>Graph API</b>, это высокоуровневый <i>HTTP</i> клиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from facebook import GraphAPI\n",
    "\n",
    "\n",
    "graph = GraphAPI(access_token=FACEBOOK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же нам понадобится модуль для совершения <b>HTTP</b> запросов. Необходим для тех действий, которые недоступны в <b>Graph API</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загружающая последовательно посты пока не достигнет предела группы или ограничения по кол-ву (константа <b>FACEBOOK_POSTS_COUNT</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_next_posts(posts, max_count):\n",
    "    result = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        if count > max_count: # Если зугрузили необходимое кол-во то прекращаем работу\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            for post_data in posts['data']: # Под ключем 'data' хранится список постов в паке\n",
    "                keys = post_data.keys()     # Так как мы нам нужны сами сообщения\n",
    "                                            \n",
    "                \n",
    "                if 'message' in keys:       # Добавляем в результат только те у которых есть\n",
    "                    result.append(post_data)# текстовое сообщение (смотрим поналичию ключа 'message')\n",
    "\n",
    "            # Меняем размер запрашиваемого пака (страницы) с постами, в уже сформированном запросе \n",
    "            # от Facebook Graph API.\n",
    "            request = posts['paging']['next'].replace(\"limit=25\", \"limit=100\")\n",
    "\n",
    "            # Выполняем запрос на получение следующей страницы с вопросами\n",
    "            s_time = time()\n",
    "            posts = requests.get(request).json()\n",
    "            f_time = time()\n",
    "            \n",
    "            posts_count = len(posts['data'])\n",
    "            count += posts_count\n",
    "            \n",
    "            print \"Время загрузки пака постов ->\", (f_time - s_time), \"|\", \"кол-во:\", posts_count, \"|\", \"всего:\", count\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    print \"Общее кол-во загруженных постов группы ->\", count\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя ранее описанный список постов заполняем нашу базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    # Получаем данные о группе по ее id на facebook.com\n",
    "    s_time = time()\n",
    "    group_json = graph.get_object(group.id)\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки данных [', group_json['name'], '] группы ->', (f_time - s_time)\n",
    "    \n",
    "    # Получаем первый пак постов, используем уже для этого метод \"get_connections\"\n",
    "    # который, грубо говоря, дает возможность запрашивать списки\n",
    "    s_time = time()\n",
    "    first_posts_pack = graph.get_connections(group_json['id'], 'feed')\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки первого пака постов ->', (f_time - s_time)\n",
    "    \n",
    "    # Последовательно загружаем все последующие посты\n",
    "    posts = load_next_posts(posts=first_posts_pack, max_count=FACEBOOK_POSTS_COUNT)\n",
    "    \n",
    "    # Сохраняем все в базу данных\n",
    "    s_time = time()\n",
    "    fdb.save_posts(group.name, group.domain, posts)\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время сохранение постов в БД ->\", (f_time - s_time)\n",
    "    print \"-------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск паттерна (шаблона) \"Имя Фамилия\"\n",
    "\n",
    "Решение в лоб - это при поступление запроса <b>\"имя_фамилия\"</b> на поиск просматривать весь список постов на наличе подстроки в их тексте шаблона <b>\"Имя Фамилия\"</b>. Главная проблема такого решения это скорость работы, так как объемы даных в которых будет осуществлятся поиск скорее всго будет достаточно большим.\n",
    "\n",
    "Мое решение заключается в следующем:<br>\n",
    "Подготовка данных, в которых будет осуществлятся поиска, перед самим поиском. Суть в том, чтобы найти все пары \"Имя фамилия\" зарание и единожды, сохранив отношения <b>\"имя фамилия\" <==> \"пост в котором было найдено имя фамилия\"</b>. А сам поиск будем осуществлять уже в этом подготовленном списке отношений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Регулярное выражение для \"Имя Фамилия\"\n",
    "FIRST_LAST_NAME_PATTERN = \"[A-Z]{1}[a-z]+\\s+[A-Z]{1}[a-z]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем пользоваться уже сохраненными данными (текст постов, которые мы сохранили в базу данных <b>Elasticsearch</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = fdb.get_all_posts() # Вытаскиваем все посты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь отищем все имена в тексте каждого поста и сразу же сохраним их в виде отношений в базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска имен по регулярному выражению -> 244\n",
      "Кол-во найденных совпадений по регулярному выраженияю: 30340\n"
     ]
    }
   ],
   "source": [
    "name_relations = []\n",
    "\n",
    "s_time = time()\n",
    "\n",
    "for post in posts:\n",
    "    message = post['_source']['message']\n",
    "    names = re.findall(FIRST_LAST_NAME_PATTERN, message)\n",
    "\n",
    "    if names:\n",
    "        id = post['_id']\n",
    "\n",
    "        for name in names:\n",
    "            name = name.replace(\".\", \"\")\n",
    "            relation = FLNameData(fl_name=name, post_id=id)\n",
    "            name_relations.append(relation)\n",
    "\n",
    "f_time = time()\n",
    "\n",
    "print \"Время поиска имен по регулярному выражению ->\", (f_time - s_time)\n",
    "print \"Кол-во найденных совпадений по регулярному выраженияю:\", len(name_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку все отношения мы так же будем хранить в базе данных, то очистим ранее созданные связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время удаления старых отношений 'сообщение <=> имя' -> 11436\n",
      "Удалено старых отношений: 60605\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "deleted_posts = fdb.delete_all_name_relations()\n",
    "f_time = time()\n",
    "\n",
    "print \"Время удаления старых отношений 'сообщение <=> имя' ->\", (f_time - s_time)\n",
    "print \"Удалено старых отношений:\", deleted_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время сохранения новых отношений 'текст поста <=> имя' -> 13640\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "fdb.save_name_relations(name_relations)\n",
    "f_time = time()\n",
    "\n",
    "print \"Время сохранения новых отношений 'текст поста <=> имя' ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда все связи сохранены, мы можем осуществить поиск.\n",
    "\n",
    "\n",
    "Ход действий таков:\n",
    "1. Сделать запрос в базу данных, что бы достать все свзязи в которых имя совпадает с запрашиваемым\n",
    "2. Получить id групп из результата (пункт 1)\n",
    "3. По найденным id групп мы теперь можем получить посты, тексты которых содержат запрашиваемые имена.\n",
    "\n",
    "Для демонстрации мы возьмем предположительно самое популярное имя <i>\"Donald Trump\"</i> (имя, которое не сходит с уст всевозможных СМИ), так как большинство сохраненных постов относятся к домену \"Политика\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя [ John Kerry ] найдено соответстивий (сообщений): 19\n",
      "Имя [ Paul Reichler ] найдено соответстивий (сообщений): 10\n",
      "Имя [ Donald Trump ] найдено соответстивий (сообщений): 185\n"
     ]
    }
   ],
   "source": [
    "# Список имен для поиска\n",
    "searched_names = [\n",
    "    \"John Kerry\",\n",
    "    \"Paul Reichler\",\n",
    "    \"Donald Trump\"\n",
    "]\n",
    "\n",
    "def find_messages_by_fl_name(fl_name):\n",
    "    finded_relations = fdb.get_name_relations_by_fl(fl_name=fl_name)\n",
    "    messages = []\n",
    "    finded_post_ids = set() # Необходимо, что бы учитывать ранее найденные посты\n",
    "\n",
    "    for relation in finded_relations:\n",
    "        post_id = relation['_source']['post_id']\n",
    "\n",
    "        if not post_id in finded_post_ids:\n",
    "            finded_post_ids.add(post_id)\n",
    "            \n",
    "            message = fdb.get_post_by_id(post_id)['_source']['message']\n",
    "            messages.append(message)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "for name in searched_names:\n",
    "    messages = find_messages_by_fl_name(name)\n",
    "    \n",
    "    print \"Имя [\", name, \"] найдено соответстивий (сообщений):\", len(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, частью подготовки документов (постов с facebook) является удаление стоп-слов (stop-words), это слова которые не несут никакой значимой информации и никак не отображают тему текста. В русском это были бы слова \"а\", \"и\", \"или\", \"в\" и тд. Эти слова встречаются в всюду и избавившись от них мы сделаем наши данные чище.\n",
    "\n",
    "Список стоп-слов для английского языка лежат в отдельном файле с иминем \"stop-words.txt\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря стоп-слов: 323 слова\n"
     ]
    }
   ],
   "source": [
    "def load_stop_words(file_name):\n",
    "    words = set()\n",
    "\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    for line in f:\n",
    "        words.add(line.strip())\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return words\n",
    "\n",
    "stop_words = load_stop_words(\"stop_words.txt\")\n",
    "print \"Размер словаря стоп-слов:\", len(stop_words), \"слова\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет <b>scikit-learn</b> уже содержит список стоп-слов для английского языка, и что бы не упустить ничего, объеденим их словарь с нашим (тот, который мы выгрузили с файла ранее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер объедененного словаря стоп-слов: 323 слова\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "\n",
    "stop_wrods = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "print \"Размер объедененного словаря стоп-слов:\", len(stop_words), \"слова\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def __init__(self):\n",
    "        super(StemmedCountVectorizer, self).__init__()\n",
    "        self.stemmer = PorterStemmer()  # проинициализируем стиммер Портера\n",
    "        self.stop_words = stop_words    # проинициализируем словарь стоп-слов\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "def create_vectorizer(): return StemmedCountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним запрос к базе данных, что бы вытащить списко всех доменов (тех, что мы записывали в базу данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domains = fdb.get_all_domains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим словарь, где ключем будет служить имя домена, а значением посты по этому домену."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время получения постов с базы данных -> 416\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "posts = dict([(domain, fdb.get_messages_by_domain(domain)) for domain in domains])\n",
    "f_time = time()\n",
    "\n",
    "print \"Время получения постов с базы данных ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во постов [ pokemon_go ]: 338\n",
      "Кол-во постов [ music      ]: 3009\n",
      "Кол-во постов [ tech       ]: 5467\n",
      "Кол-во постов [ politics   ]: 5576\n",
      "Кол-во постов [ sport      ]: 1092\n",
      "Кол-во постов [ finances   ]: 2652\n"
     ]
    }
   ],
   "source": [
    "for k, v in posts.items():\n",
    "    print \"Кол-во постов [\", \"{0: <10}\".format(str(k)), \"]:\", len(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы работать с каждым доменом независимо и в дальнейшем иметь доступ к их словарям, создаем для каждого домена свой векторизатор. Векторизаторы (в данном случае CountVectorizer) будут хранится в словаре, под ключем имени домена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Словарь => key=\"имя_домена\", value=\"векторизатор\"\n",
    "vs = dict([(domain, create_vectorizer()) for domain in domains])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторизируем с помощью векторизатора соответствующий корпус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Словарь => key=\"имя_домена\", value=\"матрица векторов соответствующего корпуса\"\n",
    "xs = dict([(domain, vs[domain].fit_transform(posts[domain])) for domain in domains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры словарей [ pokemon_go ]: 1038\n",
      "Размеры словарей [ music      ]: 3279\n",
      "Размеры словарей [ tech       ]: 7892\n",
      "Размеры словарей [ politics   ]: 34597\n",
      "Размеры словарей [ sport      ]: 3074\n",
      "Размеры словарей [ finances   ]: 6907\n"
     ]
    }
   ],
   "source": [
    "for k, v in vs.items():\n",
    "    print \"Размеры словарей [\", \"{0: <10}\".format(str(k)), \"]:\", len(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_top_tokens(matrix, vocabulary, amount, domain_name=None):\n",
    "    s_time = time()\n",
    "    \n",
    "    counts = [(word, matrix.getcol(col_num).sum()) for word, col_num in vocabulary.items()]\n",
    "    tokens = sorted(counts, key = lambda x: -x[1])[:min(amount, len(counts))]\n",
    "    tokens_res = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        tokens_res.append((token[0], i))\n",
    "    \n",
    "    f_time = time()\n",
    "    \n",
    "    if domain_name == None:\n",
    "        print \"Время поиска [\", amount, \"] популярных токенов ->\", (f_time - s_time)\n",
    "    else:\n",
    "        print \"Время поиска [\", \"{0: <10}\".format(domain_name), \"-\", amount, \"] популярных токенов ->\", (f_time - s_time)\n",
    "        \n",
    "    return tokens_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска [ politics   - 100 ] популярных токенов -> 54690\n",
      "Время поиска [ tech       - 100 ] популярных токенов -> 3021\n",
      "Время поиска [ music      - 100 ] популярных токенов -> 672\n",
      "Время поиска [ finances   - 100 ] популярных токенов -> 1721\n",
      "Время поиска [ sport      - 100 ] популярных токенов -> 412\n",
      "Время поиска [ pokemon_go - 100 ] популярных токенов -> 76\n"
     ]
    }
   ],
   "source": [
    "top_words_dict = dict([(d, find_top_tokens(xs[d], vs[d].vocabulary_, 100, d)) for d in domains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_set = dict([(d, set(w[0] for w in top_words_dict[d])) for d in domains])\n",
    "len(top_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Несколько популярных слов со списка [ pokemon_go ]:\n",
      "[(u'pokemon', 0), (u'app', 1), (u'gym', 2), (u'play', 3), (u'catch', 4)] \n",
      "\n",
      "Несколько популярных слов со списка [ music      ]:\n",
      "[(u'new', 0), (u'like', 1), (u'mtv', 2), (u'live', 3), (u'make', 4)] \n",
      "\n",
      "Несколько популярных слов со списка [ tech       ]:\n",
      "[(u'new', 0), (u'make', 1), (u'want', 2), (u'like', 3), (u'appl', 4)] \n",
      "\n",
      "Несколько популярных слов со списка [ politics   ]:\n",
      "[(u'china', 0), (u'news', 1), (u'trump', 2), (u'say', 3), (u'media', 4)] \n",
      "\n",
      "Несколько популярных слов со списка [ sport      ]:\n",
      "[(u'team', 0), (u'sport', 1), (u'like', 2), (u'nfl', 3), (u'best', 4)] \n",
      "\n",
      "Несколько популярных слов со списка [ finances   ]:\n",
      "[(u'cnnmon', 0), (u'cnntech', 1), (u'year', 2), (u'new', 3), (u'trump', 4)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in top_words_dict.items():\n",
    "    print \"Несколько популярных слов со списка [\", \"{0: <10}\".format(str(k)), \"]:\"\n",
    "    print v[:min(len(v), 5)], \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_test = [\n",
    "    \"As islanders look forward to next year, many say they hope — and even expect — Hillary Clinton to spend at least part of her summer vacations on Martha's Vineyard if she becomes president.\",\n",
    "    \"The Pentagon says staff can still play the game on their personal phones.\",\n",
    "    \"Ronald Reagan's daughter Patti Davis is citing her father's shooting as evidence that comments like Donald J. Trump's recent blast against Hillary Clinton have real-world consequences.\",\n",
    "    \"If Hillary Clinton wins in November, she will be the first former secretary of state to take over the Oval Office since 1857.\",\n",
    "    \"'The Second Amendment was put in there not just so we can go shoot skeet or go shoot trap. It was put in so we could defend our First Amendment, the freedom of speech, and also to defend ourselves against our own government,' says US Olympic skeet shooter Kim Rhode.\",\n",
    "    \"According to the Pentagon, Special Operation Forces targeted and killed ISIS leader Hafiz Sayed Khan in Afghanistan.\",\n",
    "    \"pokemon cool game play\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As islanders look forward to next year, many ... -> politics\n",
      "The Pentagon says staff can still play the ga... -> tech\n",
      "Ronald Reagan's daughter Patti Davis is citin... -> finances\n",
      "If Hillary Clinton wins in November, she will... -> finances\n",
      "'The Second Amendment was put in there not ju... -> music\n",
      "According to the Pentagon, Special Operation ... -> politics\n",
      "pokemon cool game play... -> pokemon_go\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Поиск элементов в итерируемых коллекциях\n",
    "def find_elm(iterable, predicate_):\n",
    "    for v in iterable:\n",
    "        if predicate_(v):\n",
    "            return v\n",
    "\n",
    "    return None\n",
    "\n",
    "# Для поиска категории к которой относится текст (функция find_domain(txt)), \n",
    "# нам понядобится один вектооризатор\n",
    "fd_vec = create_vectorizer()\n",
    "\n",
    "def find_domain(txt):\n",
    "    # Если список категорий пуст, то нечего искать\n",
    "    if len(domains) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Так векторизатор перезапишет свое состояние и можно будет получить список слов текста,\n",
    "    # в которых удалены стоп слова и выполнен стемминг\n",
    "    fd_vec.fit_transform([txt])\n",
    "    \n",
    "    # Теперь используя словарь данного текста - превратим его в сет,\n",
    "    # что необходимо для последующих манипуляций\n",
    "    words = [item[0] for item in fd_vec.vocabulary_.items()]\n",
    "    \n",
    "    # Сет слов текста\n",
    "    words_set = set(words)\n",
    "    \n",
    "    # Тут мы будем хранить пересечения словаря этого текста со словарями \n",
    "    # по каждой категории\n",
    "    res_buffer = []\n",
    "    \n",
    "    words_intersection_set = None\n",
    "    \n",
    "    # Сюда будем складывать результаты пересечений множеств слов текста и топ 100 по категориям\n",
    "    isds = []\n",
    "    \n",
    "    # Для каждого домена со списка\n",
    "    for domain in domains:\n",
    "        # Достаем для данного домена сет топ 100-та слов\n",
    "        domain_top_words_set = top_words_set[domain]\n",
    "        \n",
    "        # Находим пересечение словарей по категориям и словаря текста\n",
    "        words_intersection_set = words_set.intersection(domain_top_words_set)    \n",
    "        words_intersection_len = len(words_intersection_set)\n",
    "        \n",
    "        isds.append((domain, words_intersection_set))\n",
    "        res_buffer.append((domain, words_intersection_len))\n",
    "    \n",
    "    if len(res_buffer) == 1:\n",
    "        return res_buffer[0][0]\n",
    "\n",
    "    # Находим категорию с наибольшим кол-во пересечений\n",
    "    top_tuple = max(res_buffer, key=lambda t: t[1])\n",
    "    # Максимальное кол-во пересечений\n",
    "    max_count = (top_tuple)[1]\n",
    "    \n",
    "    # Формируем список из категорий с которыми одинаковое кол-во пересечений\n",
    "    res = [name for name, count in res_buffer if count == max_count]\n",
    "    \n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    \n",
    "    # Имя результирующей категории (домена)\n",
    "    domain_res = None\n",
    "    # Вес результирующей категории (домена)\n",
    "    domain_vol = -1\n",
    "    \n",
    "    for domain in res:\n",
    "        # Находим вес для каждого слова в пересечениях с разными категориями\n",
    "        # чтобы по весу сказать какая категория преобладает\n",
    "        top_words = top_words_dict[domain]\n",
    "        \n",
    "        # Вес\n",
    "        vol = 0\n",
    "        \n",
    "        words_intersection_set = find_elm(isds, lambda e: e[0] == domain)[1]\n",
    "        \n",
    "        for w in words_intersection_set:\n",
    "            vol_tmp = find_elm(top_words, lambda e: e[0] == w)[1]\n",
    "            \n",
    "            if vol_tmp != None:\n",
    "                vol += vol_tmp\n",
    "        \n",
    "        # В случае, если и вес одинаков, сравниваем имена груп\n",
    "        # и выбор делаем в сторону большей\n",
    "        if vol > domain_vol:\n",
    "            domain_res = domain\n",
    "            domain_vol = vol\n",
    "        elif vol == domain_vol:\n",
    "            if domain > domain_res:\n",
    "                domain_res = domain\n",
    "                domain_vol = vol\n",
    "    \n",
    "    return domain_res\n",
    "\n",
    "        \n",
    "for doc in docs_test:\n",
    "    domain = find_domain(doc)  \n",
    "    print doc[0:45] + \"...\", \"->\", domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
