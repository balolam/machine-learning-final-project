{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальное задание по практике\n",
    "--------------------\n",
    "\n",
    "### Цель\n",
    "\n",
    "Ознакомится с базовыми алгоритмами и базовыми инструментами машинного обучения для анализа текста\n",
    "\n",
    "### Задание\n",
    "\n",
    "1) Получить категоризованные данные с <b>Facebook API</b><br>\n",
    "2) Сохранить полученные данные<br>\n",
    "3) Токенизация данных<br>\n",
    "4) Удаление стоп-слов<br>\n",
    "5) Стемминг<br>\n",
    "6) Поиск паттерна имя-фамилия в тексте<br>\n",
    "7) Формирование списка топ <i>100</i> важных токенов по каждой категории<br>\n",
    "8) Категоризация новых текстов по наличию в них этих топ <i>100</i> токенов<br>\n",
    "9) Оценка качества<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодировка для кода на <b>Python</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для измирения времени работы некоторых участоков подключим модуль <b>\"time\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "# вернет: текущее время в миллисекундах\n",
    "def time(): return int(round(t.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранение данных\n",
    "\n",
    "Для хранения данных была выбрана база данных <b>Elasticsearch</b>, которая предоставляет удобный интерфейс взаимодействия, а так же кроссплатформенность при работе с ней, за счет того, что взаимодействие осуществляется по HTTP протоколу.\n",
    "\n",
    "Константы работы с базой данных <b>Elasticsearch</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ES_INDEX = \"fb_group_posts\"                 # Аналог базы данных в сравнении с реляционными БД, под ним будут\n",
    "                                            # хранится все данные, необходимые по заданию\n",
    "ES_POST_DOC_TYPE = \"post\"                   \n",
    "ES_NAME_RELATION_DOC_TYPE = \"name_relation\"\n",
    "\n",
    "ES_BULK_ACTIONS_SIZE = 500                  # Размер пака данных отсылаемых за раз в elasticsearch, \n",
    "                                            # необходимо для оптимизации по скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с базой данных не относится к заданию напрямую, по-этому описанию работе с ней уделено меньше внимания. Вся работа с <b>Elasticsearch</b> скрыта в ниже описанном классе <b>FacebookDBHelper</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import TransportError\n",
    "from elasticsearch import helpers\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Дата-класс, который описывает отношение наличия имени в тексте поста\n",
    "# @param: fl_name - Имя Фамилия\n",
    "# @param: post_id - id поста, в тексте которого содержится это имя и фамилия\n",
    "class FLNameData(object):\n",
    "    def __init__(self, fl_name, post_id):\n",
    "        self.fl_name = fl_name\n",
    "        self.post_id = post_id\n",
    "\n",
    "\n",
    "class FacebookDBHelper(object):\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch()\n",
    "    \n",
    "    def save_posts(self, group_name, group_domain, posts):\n",
    "        actions = []\n",
    "\n",
    "        for post in posts:\n",
    "            if 'message' in post.keys():\n",
    "                action = {\n",
    "                    \"_index\": ES_INDEX,\n",
    "                    \"_type\": ES_POST_DOC_TYPE,\n",
    "                    \"_id\": post['id'],\n",
    "                    \"_source\": {\n",
    "                        \"message\": post['message'],\n",
    "                        \"group_name\": group_name,\n",
    "                        \"group_domain\": group_domain\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "        \n",
    "    def save_name_relations(self, relations):\n",
    "        actions = []\n",
    "\n",
    "        for relation in relations:\n",
    "            action = {\n",
    "                \"_index\": ES_INDEX,\n",
    "                \"_type\": ES_NAME_RELATION_DOC_TYPE,\n",
    "                \"_source\": {\n",
    "                    \"fl_name\": relation.fl_name,\n",
    "                    \"post_id\": relation.post_id\n",
    "                }\n",
    "            }\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "\n",
    "    def __bulk_insert(self, actions):\n",
    "        actions_list = split_list(list(actions), ES_BULK_ACTIONS_SIZE)\n",
    "\n",
    "        for acts in actions_list:\n",
    "            helpers.bulk(self.es, acts)\n",
    "\n",
    "    def get_post_by_id(self, id):\n",
    "        return self.__get(doc_type=ES_POST_DOC_TYPE, id=id)\n",
    "\n",
    "    def get_all_posts(self):\n",
    "        return self.__get_all(doc_type=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def get_all_name_relations(self, doc_type):\n",
    "        relations = self.__get_all(doc_type=doc_type)\n",
    "\n",
    "        # noinspection PyTypeChecker\n",
    "        return [FLNameData(fl_name=r['_source']['fl_name'], post_id=r[\"_id\"]) for r in relations]\n",
    "\n",
    "    def __get(self, doc_type, id):\n",
    "        return self.es.get(index=ES_INDEX, doc_type=doc_type, id=id)\n",
    "\n",
    "    def __get_all(self, doc_type, body=None):\n",
    "        if body is None:\n",
    "            body = {}\n",
    "\n",
    "        result = []\n",
    "\n",
    "        page = self.es.search(\n",
    "            index=ES_INDEX,\n",
    "            doc_type=doc_type,\n",
    "            scroll='2m',\n",
    "            search_type='scan',\n",
    "            size=1000,\n",
    "            body=body)\n",
    "\n",
    "        scroll_id = page['_scroll_id']\n",
    "        scroll_size = page['hits']['total']\n",
    "\n",
    "        while scroll_size > 0:\n",
    "            page = self.es.scroll(scroll_id=scroll_id, scroll='2m')\n",
    "\n",
    "            scroll_id = page['_scroll_id']\n",
    "            scroll_size = len(page['hits']['hits'])\n",
    "\n",
    "            result.extend(page['hits']['hits'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_sources(self, doc_type):\n",
    "        posts = self.get_all_posts()\n",
    "        result = []\n",
    "\n",
    "        for post in posts:\n",
    "            if \"_source\" in post.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(post[\"_source\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_messages(self):\n",
    "        sources = self.get_all_sources(doc_type=ES_POST_DOC_TYPE)\n",
    "        result = []\n",
    "\n",
    "        for source in sources:\n",
    "            if 'message' in source.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(source[\"message\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def delete_all_posts(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def delete_all_name_relations(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_NAME_RELATION_DOC_TYPE)\n",
    "\n",
    "    def get_messages_by_domain(self, domain):\n",
    "        posts = self.__get_all(doc_type=ES_POST_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"group_domain\": {\n",
    "                        \"query\": domain,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return [p['_source']['message'] for p in posts]\n",
    "\n",
    "    def get_name_relations_by_fl(self, fl_name):\n",
    "        return self.__get_all(doc_type=ES_NAME_RELATION_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"fl_name\": {\n",
    "                        \"query\": fl_name,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    def get_all_domains(self):\n",
    "        response = self.es.search(index=ES_INDEX, doc_type=ES_POST_DOC_TYPE, body={\n",
    "            \"size\": 0,\n",
    "            \"aggs\": {\n",
    "                \"langs\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"group_domain\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for bucket in response['aggregations']['langs']['buckets']:\n",
    "            result.append(bucket['key'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def delete_by_doc_type(es, index, type_):\n",
    "    try:\n",
    "        count = es.count(index, type_)['count']\n",
    "        max_count = 5000\n",
    "\n",
    "        if not count:\n",
    "            return 0\n",
    "\n",
    "        tmp_count = count\n",
    "\n",
    "        while tmp_count > 0:\n",
    "            tmp_count -= max_count\n",
    "\n",
    "            response = es.search(\n",
    "                index=index,\n",
    "                filter_path=[\"hits.hits._id\"],\n",
    "                body={\"size\": max_count,\n",
    "                      \"query\": {\n",
    "                          \"filtered\": {\n",
    "                              \"filter\": {\n",
    "                                    \"type\": {\"value\": type_}\n",
    "                              }\n",
    "                          }\n",
    "                      }})\n",
    "\n",
    "            if not response:\n",
    "                return 0\n",
    "\n",
    "            ids = [x[\"_id\"] for x in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "            if not ids:\n",
    "                return 0\n",
    "\n",
    "            bulk_body = [\n",
    "                '{{\"delete\": {{\"_index\": \"{}\", \"_type\": \"{}\", \"_id\": \"{}\"}}}}'.format(index, type_, x)\n",
    "                for x in ids]\n",
    "\n",
    "            es.bulk('\\n'.join(bulk_body))\n",
    "            es.indices.flush_synced([index])\n",
    "\n",
    "        return count\n",
    "    except TransportError as ex:\n",
    "        print(\"Elasticsearch error: \" + ex.error)\n",
    "        raise ex\n",
    "        \n",
    "def split_list(list_, count_):\n",
    "    result = []\n",
    "\n",
    "    if len(list_) == 0:\n",
    "        return result\n",
    "\n",
    "    if len(list_) == count_:\n",
    "        result.append(list_)\n",
    "\n",
    "        return result\n",
    "\n",
    "    steps = len(list_) / count_\n",
    "    tmp_list = deepcopy(list_)\n",
    "\n",
    "    for i in range(0, steps):\n",
    "        result.append(tmp_list[0: count_])\n",
    "        tmp_list = tmp_list[count_: len(tmp_list)]\n",
    "\n",
    "    if len(tmp_list) != 0:\n",
    "        result.append(tmp_list)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экземпляр класса <b>FacebookDBHelper</b>, через который мы будем взаимодействовать с <b>Elasticsearch</b> во всей работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdb = FacebookDBHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка постов с facebook.com используя Gaph API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемые константы для работы с <b>Facebook API</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Токен зарегестрированного приложения в консоли разработчика на facebook.com\n",
    "FACEBOOK_TOKEN = \"1769775703259571|736fc7f9c5dc31707d40709a1d37813b\"\n",
    "\n",
    "# Кол-во постов загружаемых с одной группы на facebook.com\n",
    "FACEBOOK_POSTS_COUNT = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сущьность описывающая заведомо известные данные о группе в <b>Facebook</b>:\n",
    "* Имя\n",
    "* Id на facebook.com\n",
    "* Домен (класс) тематики группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    def __init__(self, name, id, domain):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.domain = domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список групп, которые будут использованы для работы (с которых мы будем загружать посты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "#     Group(name=\"CNN Politics\", id=\"219367258105115\", domain=\"politics\"),\n",
    "#     Group(name=\"SinoRuss\", id=\"1565161760380398\", domain=\"politics\"),\n",
    "#     Group(name=\"Politics & Sociology\", id=\"1616754815303974\", domain=\"politics\"),\n",
    "#     Group(name=\"CNN Money\", id=\"6651543066\", domain=\"finances\"),\n",
    "#     Group(name=\"MTV\", id=\"7245371700\", domain=\"music\"),\n",
    "#     Group(name=\"CNET\", id=\"7155422274\", domain=\"tech\"),\n",
    "#     Group(name=\"TechCrunch\", id=\"8062627951\", domain=\"tech\"),\n",
    "#     Group(name=\"Sport Addicts\", id=\"817513368382866\", domain=\"sport\"),\n",
    "#     Group(name=\"Pokemon GO\", id=\"1745029562403910\", domain=\"pokemon_go\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с Facebook будем использовать <b>Graph API</b>, это высокоуровневый <i>HTTP</i> клиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from facebook import GraphAPI\n",
    "\n",
    "graph = GraphAPI(access_token=FACEBOOK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же нам понадобится модуль для совершения <b>HTTP</b> запросов. Необходим для тех действий, которые недоступны в <b>Graph API</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загружающая последовательно посты пока не достигнет предела группы или ограничения по кол-ву (константа <b>FACEBOOK_POSTS_COUNT</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_next_posts(posts, max_count):\n",
    "    result = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        if count > max_count: # Если зугрузили необходимое кол-во то прекращаем работу\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            for post_data in posts['data']: # Под ключем 'data' хранится список постов в паке\n",
    "                keys = post_data.keys()     # Так как мы нам нужны сами сообщения\n",
    "                                            \n",
    "                \n",
    "                if 'message' in keys:       # Добавляем в результат только те у которых есть\n",
    "                    result.append(post_data)# текстовое сообщение (смотрим поналичию ключа 'message')\n",
    "\n",
    "            # Меняем размер запрашиваемого пака (страницы) с постами, в уже сформированном запросе \n",
    "            # от Facebook Graph API.\n",
    "            request = posts['paging']['next'].replace(\"limit=25\", \"limit=100\")\n",
    "\n",
    "            # Выполняем запрос на получение следующей страницы с вопросами\n",
    "            s_time = time()\n",
    "            posts = requests.get(request).json()\n",
    "            f_time = time()\n",
    "            \n",
    "            posts_count = len(posts['data'])\n",
    "            count += posts_count\n",
    "            \n",
    "            print \"Время загрузки пака постов ->\", (f_time - s_time), \"|\", \"кол-во:\", posts_count, \"|\", \"всего:\", count\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    print \"Общее кол-во загруженных постов группы ->\", count\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя ранее описанный список постов заполняем нашу базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время загрузки данных [ CNN Politics ] группы -> 1742\n",
      "Время загрузки первого пака постов -> 781\n",
      "Время загрузки пака постов -> 1150 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1090 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 2206 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 2346 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 1842 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 2051 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 1738 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 1638 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 1882 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 1805 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 2571 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 1923 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 2368 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 2181 | кол-во: 100 | всего: 1400\n",
      "Время загрузки пака постов -> 2009 | кол-во: 100 | всего: 1500\n",
      "Время загрузки пака постов -> 2933 | кол-во: 100 | всего: 1600\n",
      "Время загрузки пака постов -> 2086 | кол-во: 100 | всего: 1700\n",
      "Время загрузки пака постов -> 2154 | кол-во: 100 | всего: 1800\n",
      "Время загрузки пака постов -> 1956 | кол-во: 100 | всего: 1900\n",
      "Время загрузки пака постов -> 2550 | кол-во: 100 | всего: 2000\n",
      "Время загрузки пака постов -> 2541 | кол-во: 100 | всего: 2100\n",
      "Время загрузки пака постов -> 1547 | кол-во: 100 | всего: 2200\n",
      "Время загрузки пака постов -> 1644 | кол-во: 100 | всего: 2300\n",
      "Время загрузки пака постов -> 1865 | кол-во: 100 | всего: 2400\n",
      "Время загрузки пака постов -> 3152 | кол-во: 100 | всего: 2500\n",
      "Время загрузки пака постов -> 2042 | кол-во: 100 | всего: 2600\n",
      "Время загрузки пака постов -> 1537 | кол-во: 100 | всего: 2700\n",
      "Время загрузки пака постов -> 1795 | кол-во: 100 | всего: 2800\n",
      "Время загрузки пака постов -> 2443 | кол-во: 100 | всего: 2900\n",
      "Время загрузки пака постов -> 2325 | кол-во: 100 | всего: 3000\n",
      "Время загрузки пака постов -> 1977 | кол-во: 100 | всего: 3100\n",
      "Общее кол-во загруженных постов группы -> 3100\n",
      "Время сохранение постов в БД -> 2659\n",
      "-------------------------\n",
      "Время загрузки данных [ International Friends of China & Russia ] группы -> 768\n",
      "Время загрузки первого пака постов -> 1097\n",
      "Время загрузки пака постов -> 1784 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1831 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 1787 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 3653 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 8033 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 7681 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 2112 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 2461 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 2045 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 2352 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 2840 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 6271 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 7235 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 4165 | кол-во: 98 | всего: 1398\n",
      "Время загрузки пака постов -> 8618 | кол-во: 100 | всего: 1498\n",
      "Время загрузки пака постов -> 2831 | кол-во: 100 | всего: 1598\n",
      "Время загрузки пака постов -> 4931 | кол-во: 87 | всего: 1685\n",
      "Время загрузки пака постов -> 1751 | кол-во: 100 | всего: 1785\n",
      "Время загрузки пака постов -> 3798 | кол-во: 100 | всего: 1885\n",
      "Время загрузки пака постов -> 2331 | кол-во: 99 | всего: 1984\n",
      "Время загрузки пака постов -> 1748 | кол-во: 100 | всего: 2084\n",
      "Время загрузки пака постов -> 1324 | кол-во: 100 | всего: 2184\n",
      "Время загрузки пака постов -> 2362 | кол-во: 100 | всего: 2284\n",
      "Время загрузки пака постов -> 7618 | кол-во: 100 | всего: 2384\n",
      "Время загрузки пака постов -> 3007 | кол-во: 100 | всего: 2484\n",
      "Время загрузки пака постов -> 1902 | кол-во: 100 | всего: 2584\n",
      "Время загрузки пака постов -> 1883 | кол-во: 100 | всего: 2684\n",
      "Время загрузки пака постов -> 2666 | кол-во: 100 | всего: 2784\n",
      "Время загрузки пака постов -> 3431 | кол-во: 100 | всего: 2884\n",
      "Время загрузки пака постов -> 3662 | кол-во: 100 | всего: 2984\n",
      "Время загрузки пака постов -> 2683 | кол-во: 100 | всего: 3084\n",
      "Общее кол-во загруженных постов группы -> 3084\n",
      "Время сохранение постов в БД -> 1512\n",
      "-------------------------\n",
      "Время загрузки данных [ Politics & Sociology: Afterbirth ] группы -> 1334\n",
      "Время загрузки первого пака постов -> 1561\n",
      "Время загрузки пака постов -> 7162 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 2465 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 6784 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 1703 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 1146 | кол-во: 73 | всего: 473\n",
      "Время загрузки пака постов -> 1233 | кол-во: 56 | всего: 529\n",
      "Время загрузки пака постов -> 921 | кол-во: 66 | всего: 595\n",
      "Время загрузки пака постов -> 1629 | кол-во: 76 | всего: 671\n",
      "Время загрузки пака постов -> 1036 | кол-во: 59 | всего: 730\n",
      "Время загрузки пака постов -> 7258 | кол-во: 64 | всего: 794\n",
      "Время загрузки пака постов -> 3275 | кол-во: 81 | всего: 875\n",
      "Время загрузки пака постов -> 6240 | кол-во: 78 | всего: 953\n",
      "Время загрузки пака постов -> 5980 | кол-во: 87 | всего: 1040\n",
      "Время загрузки пака постов -> 1088 | кол-во: 81 | всего: 1121\n",
      "Время загрузки пака постов -> 1695 | кол-во: 22 | всего: 1143\n",
      "Время загрузки пака постов -> 1058 | кол-во: 0 | всего: 1143\n",
      "Общее кол-во загруженных постов группы -> 1143\n",
      "Время сохранение постов в БД -> 476\n",
      "-------------------------\n",
      "Время загрузки данных [ CNNMoney ] группы -> 642\n",
      "Время загрузки первого пака постов -> 800\n",
      "Время загрузки пака постов -> 1155 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1227 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 1242 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 2132 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 1578 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 4998 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 6930 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 2562 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 3375 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 2181 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 2148 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 2221 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 1939 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 1635 | кол-во: 100 | всего: 1400\n",
      "Время загрузки пака постов -> 1641 | кол-во: 100 | всего: 1500\n",
      "Время загрузки пака постов -> 2890 | кол-во: 100 | всего: 1600\n",
      "Время загрузки пака постов -> 3765 | кол-во: 100 | всего: 1700\n",
      "Время загрузки пака постов -> 2542 | кол-во: 100 | всего: 1800\n",
      "Время загрузки пака постов -> 3089 | кол-во: 100 | всего: 1900\n",
      "Время загрузки пака постов -> 10141 | кол-во: 100 | всего: 2000\n",
      "Время загрузки пака постов -> 2637 | кол-во: 100 | всего: 2100\n",
      "Время загрузки пака постов -> 7577 | кол-во: 100 | всего: 2200\n",
      "Время загрузки пака постов -> 1854 | кол-во: 100 | всего: 2300\n",
      "Время загрузки пака постов -> 3706 | кол-во: 100 | всего: 2400\n",
      "Время загрузки пака постов -> 1913 | кол-во: 100 | всего: 2500\n",
      "Время загрузки пака постов -> 3992 | кол-во: 100 | всего: 2600\n",
      "Время загрузки пака постов -> 7482 | кол-во: 100 | всего: 2700\n",
      "Время загрузки пака постов -> 2655 | кол-во: 100 | всего: 2800\n",
      "Время загрузки пака постов -> 3173 | кол-во: 100 | всего: 2900\n",
      "Время загрузки пака постов -> 2763 | кол-во: 100 | всего: 3000\n",
      "Время загрузки пака постов -> 2764 | кол-во: 100 | всего: 3100\n",
      "Общее кол-во загруженных постов группы -> 3100\n",
      "Время сохранение постов в БД -> 1549\n",
      "-------------------------\n",
      "Время загрузки данных [ MTV ] группы -> 1316\n",
      "Время загрузки первого пака постов -> 1229\n",
      "Время загрузки пака постов -> 1535 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1235 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 6468 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 1604 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 2267 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 3578 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 3880 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 4224 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 3470 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 6762 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 3275 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 3765 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 7174 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 2712 | кол-во: 100 | всего: 1400\n",
      "Время загрузки пака постов -> 3578 | кол-во: 100 | всего: 1500\n",
      "Время загрузки пака постов -> 8119 | кол-во: 100 | всего: 1600\n",
      "Время загрузки пака постов -> 2781 | кол-во: 100 | всего: 1700\n",
      "Время загрузки пака постов -> 6654 | кол-во: 100 | всего: 1800\n",
      "Время загрузки пака постов -> 1945 | кол-во: 100 | всего: 1900\n",
      "Время загрузки пака постов -> 2876 | кол-во: 100 | всего: 2000\n",
      "Время загрузки пака постов -> 2577 | кол-во: 100 | всего: 2100\n",
      "Время загрузки пака постов -> 2527 | кол-во: 100 | всего: 2200\n",
      "Время загрузки пака постов -> 7296 | кол-во: 100 | всего: 2300\n",
      "Время загрузки пака постов -> 2649 | кол-во: 100 | всего: 2400\n",
      "Время загрузки пака постов -> 2323 | кол-во: 100 | всего: 2500\n",
      "Время загрузки пака постов -> 8210 | кол-во: 100 | всего: 2600\n",
      "Время загрузки пака постов -> 2886 | кол-во: 100 | всего: 2700\n",
      "Время загрузки пака постов -> 2541 | кол-во: 100 | всего: 2800\n",
      "Время загрузки пака постов -> 2043 | кол-во: 100 | всего: 2900\n",
      "Время загрузки пака постов -> 3275 | кол-во: 100 | всего: 3000\n",
      "Время загрузки пака постов -> 3516 | кол-во: 100 | всего: 3100\n",
      "Общее кол-во загруженных постов группы -> 3100\n",
      "Время сохранение постов в БД -> 1662\n",
      "-------------------------\n",
      "Время загрузки данных [ CNET ] группы -> 1578\n",
      "Время загрузки первого пака постов -> 1969\n",
      "Время загрузки пака постов -> 1413 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1839 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 1639 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 2963 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 1738 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 2058 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 2763 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 2451 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 6652 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 1855 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 1640 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 2204 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 1878 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 3338 | кол-во: 100 | всего: 1400\n",
      "Время загрузки пака постов -> 2916 | кол-во: 100 | всего: 1500\n",
      "Время загрузки пака постов -> 1935 | кол-во: 100 | всего: 1600\n",
      "Время загрузки пака постов -> 1941 | кол-во: 100 | всего: 1700\n",
      "Время загрузки пака постов -> 2382 | кол-во: 100 | всего: 1800\n",
      "Время загрузки пака постов -> 1713 | кол-во: 100 | всего: 1900\n",
      "Время загрузки пака постов -> 3272 | кол-во: 100 | всего: 2000\n",
      "Время загрузки пака постов -> 2065 | кол-во: 100 | всего: 2100\n",
      "Время загрузки пака постов -> 2238 | кол-во: 100 | всего: 2200\n",
      "Время загрузки пака постов -> 2153 | кол-во: 100 | всего: 2300\n",
      "Время загрузки пака постов -> 1836 | кол-во: 100 | всего: 2400\n",
      "Время загрузки пака постов -> 6558 | кол-во: 100 | всего: 2500\n",
      "Время загрузки пака постов -> 2561 | кол-во: 100 | всего: 2600\n",
      "Время загрузки пака постов -> 3988 | кол-во: 100 | всего: 2700\n",
      "Время загрузки пака постов -> 2655 | кол-во: 100 | всего: 2800\n",
      "Время загрузки пака постов -> 1948 | кол-во: 100 | всего: 2900\n",
      "Время загрузки пака постов -> 1950 | кол-во: 100 | всего: 3000\n",
      "Время загрузки пака постов -> 1741 | кол-во: 100 | всего: 3100\n",
      "Общее кол-во загруженных постов группы -> 3100\n",
      "Время сохранение постов в БД -> 1324\n",
      "-------------------------\n",
      "Время загрузки данных [ TechCrunch ] группы -> 1221\n",
      "Время загрузки первого пака постов -> 1236\n",
      "Время загрузки пака постов -> 1437 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 1225 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 2043 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 2514 | кол-во: 100 | всего: 400\n",
      "Время загрузки пака постов -> 2329 | кол-во: 100 | всего: 500\n",
      "Время загрузки пака постов -> 2222 | кол-во: 100 | всего: 600\n",
      "Время загрузки пака постов -> 2148 | кол-во: 100 | всего: 700\n",
      "Время загрузки пака постов -> 2250 | кол-во: 100 | всего: 800\n",
      "Время загрузки пака постов -> 3179 | кол-во: 100 | всего: 900\n",
      "Время загрузки пака постов -> 1945 | кол-во: 100 | всего: 1000\n",
      "Время загрузки пака постов -> 2147 | кол-во: 100 | всего: 1100\n",
      "Время загрузки пака постов -> 3277 | кол-во: 100 | всего: 1200\n",
      "Время загрузки пака постов -> 1841 | кол-во: 100 | всего: 1300\n",
      "Время загрузки пака постов -> 2764 | кол-во: 100 | всего: 1400\n",
      "Время загрузки пака постов -> 2452 | кол-во: 100 | всего: 1500\n",
      "Время загрузки пака постов -> 7165 | кол-во: 100 | всего: 1600\n",
      "Время загрузки пака постов -> 3078 | кол-во: 100 | всего: 1700\n",
      "Время загрузки пака постов -> 1946 | кол-во: 100 | всего: 1800\n",
      "Время загрузки пака постов -> 7059 | кол-во: 100 | всего: 1900\n",
      "Время загрузки пака постов -> 3177 | кол-во: 100 | всего: 2000\n",
      "Время загрузки пака постов -> 2053 | кол-во: 100 | всего: 2100\n",
      "Время загрузки пака постов -> 1733 | кол-во: 100 | всего: 2200\n",
      "Время загрузки пака постов -> 2044 | кол-во: 100 | всего: 2300\n",
      "Время загрузки пака постов -> 3175 | кол-во: 100 | всего: 2400\n",
      "Время загрузки пака постов -> 6659 | кол-во: 100 | всего: 2500\n",
      "Время загрузки пака постов -> 2044 | кол-во: 100 | всего: 2600\n",
      "Время загрузки пака постов -> 1950 | кол-во: 100 | всего: 2700\n",
      "Время загрузки пака постов -> 2565 | кол-во: 100 | всего: 2800\n",
      "Время загрузки пака постов -> 2009 | кол-во: 100 | всего: 2900\n",
      "Время загрузки пака постов -> 3240 | кол-во: 100 | всего: 3000\n",
      "Время загрузки пака постов -> 2270 | кол-во: 100 | всего: 3100\n",
      "Общее кол-во загруженных постов группы -> 3100\n",
      "Время сохранение постов в БД -> 1478\n",
      "-------------------------\n",
      "Время загрузки данных [ Sports Addicts ] группы -> 1002\n",
      "Время загрузки первого пака постов -> 1132\n",
      "Время загрузки пака постов -> 1648 | кол-во: 94 | всего: 94\n",
      "Время загрузки пака постов -> 3083 | кол-во: 98 | всего: 192\n",
      "Время загрузки пака постов -> 3260 | кол-во: 93 | всего: 285\n",
      "Время загрузки пака постов -> 923 | кол-во: 86 | всего: 371\n",
      "Время загрузки пака постов -> 1182 | кол-во: 81 | всего: 452\n",
      "Время загрузки пака постов -> 1243 | кол-во: 87 | всего: 539\n",
      "Время загрузки пака постов -> 6114 | кол-во: 87 | всего: 626\n",
      "Время загрузки пака постов -> 1449 | кол-во: 94 | всего: 720\n",
      "Время загрузки пака постов -> 1327 | кол-во: 100 | всего: 820\n",
      "Время загрузки пака постов -> 1428 | кол-во: 90 | всего: 910\n",
      "Время загрузки пака постов -> 2277 | кол-во: 100 | всего: 1010\n",
      "Время загрузки пака постов -> 6029 | кол-во: 90 | всего: 1100\n",
      "Время загрузки пака постов -> 1319 | кол-во: 93 | всего: 1193\n",
      "Время загрузки пака постов -> 1689 | кол-во: 95 | всего: 1288\n",
      "Время загрузки пака постов -> 1393 | кол-во: 93 | всего: 1381\n",
      "Время загрузки пака постов -> 1242 | кол-во: 51 | всего: 1432\n",
      "Время загрузки пака постов -> 1333 | кол-во: 0 | всего: 1432\n",
      "Общее кол-во загруженных постов группы -> 1432\n",
      "Время сохранение постов в БД -> 743\n",
      "-------------------------\n",
      "Время загрузки данных [ Pokémon GO Malta - Official ] группы -> 1696\n",
      "Время загрузки первого пака постов -> 2608\n",
      "Время загрузки пака постов -> 1883 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 989 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 2988 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 1348 | кол-во: 72 | всего: 372\n",
      "Время загрузки пака постов -> 915 | кол-во: 0 | всего: 372\n",
      "Общее кол-во загруженных постов группы -> 372\n",
      "Время сохранение постов в БД -> 323\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    # Получаем данные о группе по ее id на facebook.com\n",
    "    s_time = time()\n",
    "    group_json = graph.get_object(group.id)\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки данных [', group_json['name'], '] группы ->', (f_time - s_time)\n",
    "    \n",
    "    # Получаем первый пак постов, используем уже для этого метод \"get_connections\"\n",
    "    # который, грубо говоря, дает возможность запрашивать списки\n",
    "    s_time = time()\n",
    "    first_posts_pack = graph.get_connections(group_json['id'], 'feed')\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки первого пака постов ->', (f_time - s_time)\n",
    "    \n",
    "    # Последовательно загружаем все последующие посты\n",
    "    posts = load_next_posts(posts=first_posts_pack, max_count=FACEBOOK_POSTS_COUNT)\n",
    "    \n",
    "    # Сохраняем все в базу данных\n",
    "    s_time = time()\n",
    "    fdb.save_posts(group.name, group.domain, posts)\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время сохранение постов в БД ->\", (f_time - s_time)\n",
    "    print \"-------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск паттерна (шаблона) \"Имя Фамилия\"\n",
    "\n",
    "Решение в лоб - это при поступление запроса <b>\"имя_фамилия\"</b> на поиск просматривать весь список постов на наличе подстроки в их тексте шаблона <b>\"Имя Фамилия\"</b>. Главная проблема такого решения это скорость работы, так как объемы даных в которых будет осуществлятся поиск скорее всго будет достаточно большим.\n",
    "\n",
    "Мое решение заключается в следующем:<br>\n",
    "Подготовка данных, в которых будет осуществлятся поиска, перед самим поиском. Суть в том, чтобы найти все пары \"Имя фамилия\" зарание и единожды, сохранив отношения <b>\"имя фамилия\" <==> \"пост в котором было найдено имя фамилия\"</b>. А сам поиск будем осуществлять уже в этом подготовленном списке отношений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Регулярное выражение для \"Имя Фамилия\"\n",
    "FIRST_LAST_NAME_PATTERN = \"[A-Z]{1}[a-z]+\\s+[A-Z]{1}[a-z]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем пользоваться уже сохраненными данными (пост, которые мы сохранили в базу данных <b>Elasticsearch</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = fdb.get_all_posts() # Вытаскиваем все посты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь отищем все имена в тексте каждого поста и сразу же сохраним их в виде отношений в базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска имен по регулярному выражению -> 290\n",
      "Кол-во найденных совпадений по регулярному выраженияю: 30340\n"
     ]
    }
   ],
   "source": [
    "name_relations = []\n",
    "\n",
    "s_time = time()\n",
    "\n",
    "for post in posts:\n",
    "    message = post['_source']['message']\n",
    "    names = re.findall(FIRST_LAST_NAME_PATTERN, message)\n",
    "\n",
    "    if names:\n",
    "        id = post['_id']\n",
    "\n",
    "        for name in names:\n",
    "            name = name.replace(\".\", \"\")\n",
    "            relation = FLNameData(fl_name=name, post_id=id)\n",
    "            name_relations.append(relation)\n",
    "\n",
    "f_time = time()\n",
    "\n",
    "print \"Время поиска имен по регулярному выражению ->\", (f_time - s_time)\n",
    "print \"Кол-во найденных совпадений по регулярному выраженияю:\", len(name_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку все отношения мы так же будем хранить в базе данных, то очистим ранее созданные связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время удаления старых отношений 'сообщение <=> имя' -> 4\n",
      "Удалено старых отношений: 0\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "deleted_posts = fdb.delete_all_name_relations()\n",
    "f_time = time()\n",
    "\n",
    "print \"Время удаления старых отношений 'сообщение <=> имя' ->\", (f_time - s_time)\n",
    "print \"Удалено старых отношений:\", deleted_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время сохранения новых отношений 'текст поста <=> имя' -> 16060\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "fdb.save_name_relations(name_relations)\n",
    "f_time = time()\n",
    "\n",
    "print \"Время сохранения новых отношений 'текст поста <=> имя' ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает поиск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя [ John Kerry ] найдено соответстивий (сообщений): 19\n",
      "Имя [ Paul Reichler ] найдено соответстивий (сообщений): 10\n",
      "Имя [ Donald Trump ] найдено соответстивий (сообщений): 185\n"
     ]
    }
   ],
   "source": [
    "searched_names = [\n",
    "    \"John Kerry\",\n",
    "    \"Paul Reichler\",\n",
    "    \"Donald Trump\"\n",
    "]\n",
    "\n",
    "def find_messages_by_fl_name(fl_name):\n",
    "    finded_relations = fdb.get_name_relations_by_fl(fl_name=fl_name)\n",
    "    messages = []\n",
    "    finded_post_ids = set() # Необходимо, что бы учитывать ранее найденные посты\n",
    "\n",
    "    for relation in finded_relations:\n",
    "        post_id = relation['_source']['post_id']\n",
    "\n",
    "        if not post_id in finded_post_ids:\n",
    "            finded_post_ids.add(post_id)\n",
    "            \n",
    "            message = fdb.get_post_by_id(post_id)['_source']['message']\n",
    "            messages.append(message)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "for name in searched_names:\n",
    "    messages = find_messages_by_fl_name(name)\n",
    "    \n",
    "    print \"Имя [\", name, \"] найдено соответстивий (сообщений):\", len(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, частью подготовки документов (постов с facebook) является удаление стоп-слов (stop-words), это слова которые не несут никакой значимой информации и никак не отображают тему текста. В русском это были бы слова \"а\", \"и\", \"или\", \"в\" и тд. Эти слова встречаются в всюду и избавившись от них мы сделаем наши данные чище.\n",
    "\n",
    "Список стоп-слов для английского языка лежат в отдельном файле с иминем \"stop-words.txt\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря стоп-слов: 323 слова\n"
     ]
    }
   ],
   "source": [
    "def load_stop_words(file_name):\n",
    "    words = set()\n",
    "\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    for line in f:\n",
    "        words.add(line.strip())\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return words\n",
    "\n",
    "stop_words = load_stop_words(\"stop_words.txt\")\n",
    "print \"Размер словаря стоп-слов:\", len(stop_words), \"слова\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет <b>scikit-learn</b> уже содержит список стоп-слов для английского языка, и что бы не упустить ничего, объеденим их словарь с нашим (тот, который мы выгрузили с файла ранее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер объедененного словаря стоп-слов: 323 слова\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "\n",
    "stop_wrods = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "print \"Размер объедененного словаря стоп-слов:\", len(stop_words), \"слова\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def __init__(self):\n",
    "        super(StemmedCountVectorizer, self).__init__()\n",
    "        self.stemmer = PorterStemmer()  # проинициализируем стиммер  стиммер Портера\n",
    "        self.stop_words = stop_words    # проинициализируем словарь стоп-слов\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "def create_vectorizer(): return StemmedCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domains = fdb.get_all_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время получения постов с базы данных -> 369\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "posts = dict([(domain, fdb.get_messages_by_domain(domain)) for domain in domains])\n",
    "f_time = time()\n",
    "\n",
    "print \"Время получения постов с базы данных ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во постов [ pokemon_go ]: 338\n",
      "Кол-во постов [ music      ]: 3009\n",
      "Кол-во постов [ tech       ]: 5467\n",
      "Кол-во постов [ politics   ]: 5576\n",
      "Кол-во постов [ sport      ]: 1092\n",
      "Кол-во постов [ finances   ]: 2652\n"
     ]
    }
   ],
   "source": [
    "for k, v in posts.items():\n",
    "    print \"Кол-во постов [\", \"{0: <10}\".format(str(k)), \"]:\", len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vs = dict([(domain, create_vectorizer()) for domain in domains])\n",
    "xs = dict([(domain, vs[domain].fit_transform(posts[domain])) for domain in domains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры словарей [ pokemon_go ]: 1038\n",
      "Размеры словарей [ music      ]: 3279\n",
      "Размеры словарей [ tech       ]: 7892\n",
      "Размеры словарей [ politics   ]: 34597\n",
      "Размеры словарей [ sport      ]: 3074\n",
      "Размеры словарей [ finances   ]: 6907\n"
     ]
    }
   ],
   "source": [
    "for k, v in vs.items():\n",
    "    print \"Размеры словарей [\", \"{0: <10}\".format(str(k)), \"]:\", len(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_top_tokens(matrix, items, amount):\n",
    "    s_time = time()\n",
    "    counts = [(word, matrix.getcol(col_num).sum()) for word, col_num in items]\n",
    "    tokens = sorted (counts, key = lambda x: -x[1])[:min(amount, len(counts))]\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время поиска [\", amount, \"] популярных токенов ->\", (f_time - s_time)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска [ 100 ] популярных токенов -> 53792\n",
      "Время поиска [ 100 ] популярных токенов -> 2954\n",
      "Время поиска [ 100 ] популярных токенов -> 660\n",
      "Время поиска [ 100 ] популярных токенов -> 1718\n",
      "Время поиска [ 100 ] популярных токенов -> 396\n",
      "Время поиска [ 100 ] популярных токенов -> 73\n"
     ]
    }
   ],
   "source": [
    "pw = dict([(domain, find_top_tokens(xs[domain], vs[domain].vocabulary_.items(), 100)) for domain in domains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Несколько популярных слов со списка [ pokemon_go ]:\n",
      "[(u'pokemon', 74), (u'app', 20), (u'gym', 18), (u'play', 17), (u'catch', 17)] \n",
      "\n",
      "Несколько популярных слов со списка [ music      ]:\n",
      "[(u'new', 139), (u'like', 135), (u'mtv', 106), (u'live', 104), (u'make', 94)] \n",
      "\n",
      "Несколько популярных слов со списка [ tech       ]:\n",
      "[(u'new', 347), (u'make', 215), (u'want', 206), (u'like', 206), (u'appl', 205)] \n",
      "\n",
      "Несколько популярных слов со списка [ politics   ]:\n",
      "[(u'china', 2380), (u'news', 1797), (u'trump', 1619), (u'say', 1475), (u'media', 1401)] \n",
      "\n",
      "Несколько популярных слов со списка [ sport      ]:\n",
      "[(u'team', 162), (u'sport', 135), (u'like', 108), (u'nfl', 82), (u'best', 81)] \n",
      "\n",
      "Несколько популярных слов со списка [ finances   ]:\n",
      "[(u'cnnmon', 2576), (u'cnntech', 321), (u'year', 180), (u'new', 170), (u'trump', 167)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in pw.items():\n",
    "    print \"Несколько популярных слов со списка [\", \"{0: <10}\".format(str(k)), \"]:\"\n",
    "    print v[:min(len(v), 5)], \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Превращаем списки самых популярных слов в <b>SET-ы</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pws = dict([(domain, set([p1 for p1, p2 in pw[domain]])) for domain in domains])\n",
    "len(pws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_test = [\n",
    "    \"As islanders look forward to next year, many say they hope — and even expect — Hillary Clinton to spend at least part of her summer vacations on Martha's Vineyard if she becomes president.\",\n",
    "    \"The Pentagon says staff can still play the game on their personal phones.\",\n",
    "    \"Ronald Reagan's daughter Patti Davis is citing her father's shooting as evidence that comments like Donald J. Trump's recent blast against Hillary Clinton have real-world consequences.\",\n",
    "    \"If Hillary Clinton wins in November, she will be the first former secretary of state to take over the Oval Office since 1857.\",\n",
    "    \"Donald J. Trump and the billionaire Italian media mogul turned politician are both known for their larger than life personalities, their skills in capturing the attention of the press and shaping narratives in the media. Silvio Berlusconi served as Italy's prime minister for a total of nine years.\",\n",
    "    \"'The Second Amendment was put in there not just so we can go shoot skeet or go shoot trap. It was put in so we could defend our First Amendment, the freedom of speech, and also to defend ourselves against our own government,' says US Olympic skeet shooter Kim Rhode.\",\n",
    "    \"According to the Pentagon, Special Operation Forces targeted and killed ISIS leader Hafiz Sayed Khan in Afghanistan.\",\n",
    "    \"pokemon cool game play\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As islanders look forward to next year, many ... -> [u'politics', u'finances']\n",
      "The Pentagon says staff can still play the ga... -> [u'tech']\n",
      "Ronald Reagan's daughter Patti Davis is citin... -> [u'finances']\n",
      "If Hillary Clinton wins in November, she will... -> [u'politics', u'finances']\n",
      "Donald J. Trump and the billionaire Italian m... -> [u'politics', u'music', u'finances']\n",
      "'The Second Amendment was put in there not ju... -> [u'politics', u'music', u'finances']\n",
      "According to the Pentagon, Special Operation ... -> [u'politics']\n",
      "pokemon cool game play... -> [u'pokemon_go']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "fd_vec = create_vectorizer()\n",
    "\n",
    "\n",
    "def find_domain(txt):\n",
    "    fd_vec.fit_transform([txt])\n",
    "    \n",
    "    words = [p1 for p1, p2 in fd_vec.vocabulary_.items()]\n",
    "    words_set = set(words)\n",
    "    \n",
    "    res_buffer = []\n",
    "    \n",
    "    for domain in domains:\n",
    "        domain_top_words_set = pws[domain]\n",
    "        \n",
    "        words_intersection_set = words_set.intersection(domain_top_words_set)    \n",
    "        words_intersection_len = len(words_intersection_set)\n",
    "        \n",
    "        res_buffer.append((domain, words_intersection_len))\n",
    "    \n",
    "    res_buffer_len = len(res_buffer)\n",
    "    \n",
    "    if res_buffer_len == 0:\n",
    "        return \"not found\"\n",
    "    \n",
    "    if res_buffer_len == 1:\n",
    "        return res_buffer[0][0]\n",
    "\n",
    "    top_tuple = max(res_buffer, key=lambda t: t[1])\n",
    "    max_count = (top_tuple)[1]\n",
    "    \n",
    "    res = [name for name, count in res_buffer if count == max_count]\n",
    "    \n",
    "    return res\n",
    "\n",
    "        \n",
    "for doc in docs_test:\n",
    "    print doc[0:45] + \"...\", \"->\", find_domain(doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
