{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Константы работы с базой данных <b>Elasticsearch</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ES_INDEX = \"fb_group_posts\"\n",
    "ES_POST_DOC_TYPE = \"post\"\n",
    "ES_NAME_RELATION_DOC_TYPE = \"name_relation\"\n",
    "ES_BULK_ACTIONS_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся работа с Elasticsearch скрыта в классе <b>FacebookDBHelper</b>, и предоставляет удобный интерфейс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import TransportError\n",
    "from elasticsearch import helpers\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class PostData(object):\n",
    "    def __init__(self, id_, messages):\n",
    "        self.id = id_\n",
    "        self.message = messages\n",
    "\n",
    "\n",
    "class FLNameData(object):\n",
    "    def __init__(self, fl_name, post_id):\n",
    "        self.fl_name = fl_name\n",
    "        self.post_id = post_id\n",
    "\n",
    "\n",
    "class FacebookDBHelper(object):\n",
    "    def __init__(self):\n",
    "        self.es = Elasticsearch()\n",
    "\n",
    "    def save_posts(self, group_name, group_domain, posts):\n",
    "        actions = []\n",
    "\n",
    "        for post in posts:\n",
    "            if 'message' in post.keys():\n",
    "                action = {\n",
    "                    \"_index\": ES_INDEX,\n",
    "                    \"_type\": ES_POST_DOC_TYPE,\n",
    "                    \"_id\": post['id'],\n",
    "                    \"_source\": {\n",
    "                        \"message\": post['message'],\n",
    "                        \"group_name\": group_name,\n",
    "                        \"group_domain\": group_domain\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "\n",
    "    def save_name_relations(self, relations):\n",
    "        actions = []\n",
    "\n",
    "        for relation in relations:\n",
    "            action = {\n",
    "                \"_index\": ES_INDEX,\n",
    "                \"_type\": ES_NAME_RELATION_DOC_TYPE,\n",
    "                \"_source\": {\n",
    "                    \"fl_name\": relation.fl_name,\n",
    "                    \"post_id\": relation.post_id\n",
    "                }\n",
    "            }\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        self.__bulk_insert(actions)\n",
    "\n",
    "    def __bulk_insert(self, actions):\n",
    "        actions_list = split_list(list(actions), ES_BULK_ACTIONS_SIZE)\n",
    "\n",
    "        for acts in actions_list:\n",
    "            helpers.bulk(self.es, acts)\n",
    "\n",
    "    def get_post_by_id(self, id):\n",
    "        return self.__get(doc_type=ES_POST_DOC_TYPE, id=id)\n",
    "\n",
    "    def get_all_posts(self):\n",
    "        return self.__get_all(doc_type=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def get_all_name_relations(self, doc_type):\n",
    "        relations = self.__get_all(doc_type=doc_type)\n",
    "\n",
    "        # noinspection PyTypeChecker\n",
    "        return [FLNameData(fl_name=r['_source']['fl_name'], post_id=r[\"_id\"]) for r in relations]\n",
    "\n",
    "    def __get(self, doc_type, id):\n",
    "        return self.es.get(index=ES_INDEX, doc_type=doc_type, id=id)\n",
    "\n",
    "    def __get_all(self, doc_type, body=None):\n",
    "        if body is None:\n",
    "            body = {}\n",
    "\n",
    "        result = []\n",
    "\n",
    "        page = self.es.search(\n",
    "            index=ES_INDEX,\n",
    "            doc_type=doc_type,\n",
    "            scroll='2m',\n",
    "            search_type='scan',\n",
    "            size=1000,\n",
    "            body=body)\n",
    "\n",
    "        scroll_id = page['_scroll_id']\n",
    "        scroll_size = page['hits']['total']\n",
    "\n",
    "        while scroll_size > 0:\n",
    "            page = self.es.scroll(scroll_id=scroll_id, scroll='2m')\n",
    "\n",
    "            scroll_id = page['_scroll_id']\n",
    "            scroll_size = len(page['hits']['hits'])\n",
    "\n",
    "            result.extend(page['hits']['hits'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_sources(self, doc_type):\n",
    "        posts = self.get_all_posts(doc_type=doc_type)\n",
    "        result = []\n",
    "\n",
    "        for post in posts:\n",
    "            if \"_source\" in post.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(post[\"_source\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_messages(self, doc_type):\n",
    "        sources = self.get_all_sources(doc_type=doc_type)\n",
    "        result = []\n",
    "\n",
    "        for source in sources:\n",
    "            if 'message' in source.keys():\n",
    "                # noinspection PyTypeChecker\n",
    "                result.append(source[\"message\"])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def delete_all_posts(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_POST_DOC_TYPE)\n",
    "\n",
    "    def delete_all_name_relations(self):\n",
    "        return delete_by_doc_type(\n",
    "            es=self.es,\n",
    "            index=ES_INDEX,\n",
    "            type_=ES_NAME_RELATION_DOC_TYPE)\n",
    "\n",
    "    def get_messages_by_domain(self, domain):\n",
    "        return self.__get_all(doc_type=ES_POST_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"domain\": {\n",
    "                        \"query\": domain,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    def get_name_relations_by_fl(self, fl_name):\n",
    "        return self.__get_all(doc_type=ES_NAME_RELATION_DOC_TYPE, body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"fl_name\": {\n",
    "                        \"query\": fl_name,\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "\n",
    "def delete_by_doc_type(es, index, type_):\n",
    "    try:\n",
    "        count = es.count(index, type_)['count']\n",
    "        max_count = 5000\n",
    "\n",
    "        if not count:\n",
    "            return 0\n",
    "\n",
    "        tmp_count = count\n",
    "\n",
    "        while tmp_count > 0:\n",
    "            tmp_count -= max_count\n",
    "\n",
    "            response = es.search(\n",
    "                index=index,\n",
    "                filter_path=[\"hits.hits._id\"],\n",
    "                body={\"size\": max_count,\n",
    "                      \"query\": {\n",
    "                          \"filtered\": {\n",
    "                              \"filter\": {\n",
    "                                    \"type\": {\"value\": type_}\n",
    "                              }\n",
    "                          }\n",
    "                      }})\n",
    "\n",
    "            if not response:\n",
    "                return 0\n",
    "\n",
    "            ids = [x[\"_id\"] for x in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "            if not ids:\n",
    "                return 0\n",
    "\n",
    "            bulk_body = [\n",
    "                '{{\"delete\": {{\"_index\": \"{}\", \"_type\": \"{}\", \"_id\": \"{}\"}}}}'.format(index, type_, x)\n",
    "                for x in ids]\n",
    "\n",
    "            es.bulk('\\n'.join(bulk_body))\n",
    "            es.indices.flush_synced([index])\n",
    "\n",
    "        return count\n",
    "    except TransportError as ex:\n",
    "        print(\"Elasticsearch error: \" + ex.error)\n",
    "        raise ex\n",
    "        \n",
    "def split_list(list_, count_):\n",
    "    result = []\n",
    "\n",
    "    if len(list_) == 0:\n",
    "        return result\n",
    "\n",
    "    if len(list_) == count_:\n",
    "        result.append(list_)\n",
    "\n",
    "        return result\n",
    "\n",
    "    steps = len(list_) / count_\n",
    "    tmp_list = deepcopy(list_)\n",
    "\n",
    "    for i in range(0, steps):\n",
    "        result.append(tmp_list[0: count_])\n",
    "        tmp_list = tmp_list[count_: len(tmp_list)]\n",
    "\n",
    "    if len(tmp_list) != 0:\n",
    "        result.append(tmp_list)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdb = FacebookDBHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же для измирения времени работы некоторых участоков подключим модуль <b>\"time\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "def time():\n",
    "    return int(round(t.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемые константы для работы с <b>facebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FACEBOOK_TOKEN = \"1769775703259571|736fc7f9c5dc31707d40709a1d37813b\"\n",
    "FACEBOOK_POSTS_COUNT = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сущьность описывающая заведомо известные данные о группе в <b>Facebook</b>:\n",
    "* Имя\n",
    "* Id на facebook.com\n",
    "* Домен (класс) тематики группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    def __init__(self, name, id, domain):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.domain = domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список групп, которые будут использованы для работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "#     Group(name=\"CNN Politics\", id=\"219367258105115\", domain=\"politics\"),\n",
    "#     Group(name=\"SinoRuss\", id=\"1565161760380398\", domain=\"politics\"),\n",
    "#     Group(name=\"Politics & Sociology\", id=\"1616754815303974\", domain=\"politics\"),\n",
    "#     Group(name=\"CNN Money\", id=\"6651543066\", domain=\"finances\"),\n",
    "#     Group(name=\"MTV\", id=\"7245371700\", domain=\"music\"),\n",
    "#     Group(name=\"CNET\", id=\"7155422274\", domain=\"tech\"),\n",
    "#     Group(name=\"TechCrunch\", id=\"8062627951\", domain=\"tech\"),\n",
    "#     Group(name=\"Sport Addicts\", id=\"817513368382866\", domain=\"sport\"),\n",
    "    Group(name=\"Pokemon GO\", id=\"1745029562403910\", domain=\"pokemon go\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с Facebook будем использовать <b>Facebook Graph API</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from facebook import GraphAPI\n",
    "\n",
    "graph = GraphAPI(access_token=FACEBOOK_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же нам понадобится модуль для совершения <b>HTTP</b> запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загружающая последовательно посты пока не достигнет предела группы или ограничения по кол-ву."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_next_posts(posts, max_count):\n",
    "    result = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        if count > max_count:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            for post_data in posts['data']:\n",
    "                keys = post_data.keys()  # Так как мы нам нужны сами сообщения\n",
    "                                         # мы не включаем в результат посты без текстовых сообщений\n",
    "                if 'message' in keys:\n",
    "                    result.append(post_data)\n",
    "\n",
    "            # Меняем размер запрашиваемого пака (страницы) с постами, в уже сформированном запросе \n",
    "            # от Facebook Graph API\n",
    "            request = posts['paging']['next'].replace(\"limit=25\", \"limit=100\")\n",
    "\n",
    "            s_time = time()\n",
    "            posts = requests.get(request).json()\n",
    "            f_time = time()\n",
    "            \n",
    "            posts_count = len(posts['data'])\n",
    "            count += posts_count\n",
    "            \n",
    "            print \"Время загрузки пака постов ->\", (f_time - s_time), \"|\", \"кол-во:\", posts_count, \"|\", \"всего:\", count\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    print \"Общее кол-во загруженных постов группы ->\", count\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя ранее описанный список постов заполняем нашу базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время загрузки данных [ Pokémon GO Malta - Official ] группы -> 1282\n",
      "Время загрузки первого пака постов -> 1152\n",
      "Время загрузки пака постов -> 1701 | кол-во: 100 | всего: 100\n",
      "Время загрузки пака постов -> 3077 | кол-во: 100 | всего: 200\n",
      "Время загрузки пака постов -> 1261 | кол-во: 100 | всего: 300\n",
      "Время загрузки пака постов -> 3546 | кол-во: 70 | всего: 370\n",
      "Время загрузки пака постов -> 2197 | кол-во: 0 | всего: 370\n",
      "Общее кол-во загруженных постов группы -> 370\n",
      "Время сохранение постов в БД -> 629\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    # Получаем данные о группе по ее id на facebook.com\n",
    "    s_time = time()\n",
    "    group_json = graph.get_object(group.id)\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки данных [', group_json['name'], '] группы ->', (f_time - s_time)\n",
    "    \n",
    "    # Получаем первый пак постов, используем уже для этого метод \"get_connections\"\n",
    "    # который, грубо говоря, дает возможность запрашивать списки\n",
    "    s_time = time()\n",
    "    first_posts_pack = graph.get_connections(group_json['id'], 'feed')\n",
    "    f_time = time()\n",
    "    \n",
    "    print 'Время загрузки первого пака постов ->', (f_time - s_time)\n",
    "    \n",
    "    # Последовательно загружаем все последующие посты\n",
    "    posts = load_next_posts(posts=first_posts_pack, max_count=FACEBOOK_POSTS_COUNT)\n",
    "    \n",
    "    # Сохраняем все в базу данных\n",
    "    s_time = time()\n",
    "    fdb.save_posts(group.name, group.domain, posts)\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время сохранение постов в БД ->\", (f_time - s_time)\n",
    "    print \"-------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как двигатся дальше мы должны вытащить все имена, сформировав отношение \"имя <-> пост\", для последующего использования этих отношений при поиске. Логично, что это следует сделать перед дальнейшей обработкой и работы с постами, так как мы можем утратить часть имен в дальнейшем.\n",
    "\n",
    "Для того, что бы отискать все имена, мы воспользуемся регулярными выражениями, в данном случае, самой простой реализацией, которая даст много мусора, но в данном случае это не страшно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FIRST_LAST_NAME_PATTERN = \"[A-Z]{1}[a-z]+\\s+[A-Z]{1}[a-z]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем пользоваться уже сохраненными данными (тем что сохранили ранее в базу данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время поиска имен по шаблону -> 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29006"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = fdb.get_all_posts() # Вытаскиваем все посты\n",
    "name_relations = []\n",
    "\n",
    "s_time = time()\n",
    "\n",
    "for post in posts:\n",
    "    message = post['_source']['message']\n",
    "    names = re.findall(FIRST_LAST_NAME_PATTERN, message)\n",
    "\n",
    "    if names:\n",
    "        id = post['_id']\n",
    "\n",
    "        for name in names:\n",
    "            name = name.replace(\".\", \"\")\n",
    "            relation = FLNameData(fl_name=name, post_id=id)\n",
    "            name_relations.append(relation)\n",
    "\n",
    "f_time = time()\n",
    "\n",
    "print \"Время поиска имен по шаблону ->\", (f_time - s_time)\n",
    "    \n",
    "len(name_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку все отношения мы так же будем хранить в базе данных, то очистим ранее созданные связи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время удаления старых отношений 'сообщение <=> имя' -> 391\n",
      "Удалено старых постов: 0\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "deleted_posts = fdb.delete_all_name_relations()\n",
    "f_time = time()\n",
    "\n",
    "print \"Время удаления старых отношений 'сообщение <=> имя' ->\", (f_time - s_time)\n",
    "print \"Удалено старых постов:\", deleted_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время сохранения новых отношений 'сообщение <=> имя' -> 24143\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "fdb.save_name_relations(name_relations)\n",
    "f_time = time()\n",
    "\n",
    "print \"Время сохранения новых отношений 'сообщение <=> имя' ->\", (f_time - s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает поиск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя [ John Kerry ] найдено соответстивий (сообщений): 19\n",
      "Имя [ Paul Reichler ] найдено соответстивий (сообщений): 9\n"
     ]
    }
   ],
   "source": [
    "searched_names = [\n",
    "    \"John Kerry\",\n",
    "    \"Paul Reichler\"\n",
    "]\n",
    "\n",
    "def find_messages_by_fl_name(fl_name):\n",
    "    finded_relations = fdb.get_name_relations_by_fl(fl_name=fl_name)\n",
    "    messages = []\n",
    "    finded_post_ids = set() # Необходимо, что бы учитывать ранее найденные посты\n",
    "\n",
    "    for relation in finded_relations:\n",
    "        post_id = relation['_source']['post_id']\n",
    "\n",
    "        if not post_id in finded_post_ids:\n",
    "            finded_post_ids.add(post_id)\n",
    "            \n",
    "            message = fdb.get_post_by_id(post_id)['_source']['message']\n",
    "            messages.append(message)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "for name in searched_names:\n",
    "    messages = find_messages_by_fl_name(name)\n",
    "    \n",
    "    print \"Имя [\", name, \"] найдено соответстивий (сообщений):\", len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words_loader import StopWordsLoader\n",
    "\n",
    "stop_words = StopWordsLoader(\"stop_words\").get()\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    return stem_tokens(tokens, stemmer)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    \n",
    "    return stemmed\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = fdb.get_messages_by_domain(\"politics\")\n",
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = vectorizer.fit_transform(messages)\n",
    "\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def find_top_tokens(matrix):\n",
    "#     rows_count = matrix.getnnz()\n",
    "    \n",
    "#     if rows_count == 0:\n",
    "#         return \n",
    "    \n",
    "#     first_row = matrix[0]\n",
    "#     columns_count = first_row.getnnz()\n",
    "    \n",
    "#     result = []\n",
    "    \n",
    "#     for i in range(columns_count):\n",
    "#         s_time = time()\n",
    "#         result.append(sum(matrix[:,i])[0,0])\n",
    "#         f_time = time()\n",
    "        \n",
    "#         print \"time ->\", (f_time - s_time)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# summ = find_top_tokens(matrix)\n",
    "# summ\n",
    "\n",
    "def find_top_tokens(matrix, items, amount):\n",
    "    s_time = time()\n",
    "    counts = [(word, matrix.getcol(col_num).sum()) for word, col_num in items]\n",
    "    tokens = sorted (counts, key = lambda x: -x[1])[:min(amount, len(counts))]\n",
    "    f_time = time()\n",
    "    \n",
    "    print \"Время поиска [\", amount, \"] популярных токенов ->\", (f_time - s_time)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "top_100_tokens = find_top_tokens(matrix, vectorizer.vocabulary_.items(), 100)\n",
    "top_100_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted_by_count = sorted(tokens.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_by_count[0:100]\n",
    "\n",
    "\n",
    "    \n",
    "#     print \"{:10}\".format(token), \"->\", get_words_count(token, vectorizer.vocabulary_, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'run'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
